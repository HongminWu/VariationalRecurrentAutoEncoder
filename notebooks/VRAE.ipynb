{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arunesh/Code/miniconda3/envs/mml_env_local/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from jupyterthemes import jtplot\n",
    "from IPython.core.debugger import Tracer\n",
    "jtplot.style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " data_set_idx=0 z_dim=20 time_steps=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def clip_roll(piano_roll, time_steps=50):\n",
    "    samples = []\n",
    "    num_samples = int(piano_roll.shape[1] / time_steps)\n",
    "    for i in range(num_samples):\n",
    "        start_idx = time_steps*i\n",
    "        end_idx = (time_steps*(i+1))\n",
    "        samples.append(piano_roll[:,start_idx:end_idx])\n",
    "    return samples   \n",
    "\n",
    "def create_samples(load_root, time_steps=50, verbose=False):\n",
    "    if not os.path.isdir(load_root):\n",
    "        print(\"Invalid load_root directory.\")\n",
    "        sys.exit(0)\n",
    "        \n",
    "    samples = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(load_root):\n",
    "        for file in filenames:\n",
    "            if file.endswith('.npy'):\n",
    "                load_filepath = os.path.join(dirpath,file)\n",
    "                if verbose:\n",
    "                    print(load_filepath)\n",
    "                piano_roll = np.load(load_filepath).T\n",
    "                samples = samples + clip_roll(piano_roll,time_steps=time_steps)\n",
    "\n",
    "    return np.stack(samples)\n",
    "\n",
    "def feed_dict(batch_size, samples_type=None):\n",
    "    if samples_type == 'train':\n",
    "        indeces = np.random.randint(num_train_samples, size=batch_size)\n",
    "        samples = train_samples\n",
    "        \n",
    "    elif samples_type == 'valid':\n",
    "        indeces = np.random.randint(num_valid_samples, size=batch_size)\n",
    "        samples = valid_samples\n",
    "        \n",
    "    elif samples_type == 'test':\n",
    "        indeces = np.random.randint(num_test_samples, size=batch_size)\n",
    "        samples = test_samples\n",
    "        \n",
    "    return np.take(samples, indeces, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X \\in {0,1}^{batch_size, dim_x, time_steps}\n",
    "\n",
    "# Constant params\n",
    "x_in_dim = 88\n",
    "\n",
    "# Params replicated from Fabius et. al Paper\n",
    "beta_1 = 0.05\n",
    "beta_2 = 0.001\n",
    "num_epochs = 10#40000 #TEST\n",
    "learning_rate_1 = 2e-5\n",
    "learning_rate_2 = 1e-5\n",
    "num_epochs_to_diff_learn_rate = 5#16000 #TEST \n",
    "num_epochs_to_save_model = 4#1000 #TEST\n",
    "num_hidden_units = 500\n",
    "#decay_rate = .7\n",
    "\n",
    "# Hyperparams\n",
    "z_dim = 20\n",
    "time_steps = 10 \n",
    "batch_size = 100\n",
    "data_set_idx = 0\n",
    "\n",
    "# Dir paths\n",
    "data_sets = ['Nottingham','JSB Chorales']\n",
    "load_root = '../MIDI_Data_PianoRolls/'\n",
    "log_root =  '../logs/MIDI_Data_PianoRolls'\n",
    "\n",
    "train_dir_path = os.path.join(load_root,data_sets[data_set_idx],'train')\n",
    "valid_dir_path = os.path.join(load_root,data_sets[data_set_idx],'valid')\n",
    "test_dir_path = os.path.join(load_root,data_sets[data_set_idx],'test')\n",
    "\n",
    "train_samples = create_samples(train_dir_path, time_steps=time_steps, verbose=False)\n",
    "valid_samples = create_samples(valid_dir_path, time_steps=time_steps, verbose=False)\n",
    "test_samples = create_samples(test_dir_path, time_steps=time_steps, verbose=False)\n",
    "\n",
    "num_train_samples = train_samples.shape[0] \n",
    "num_valid_samples = valid_samples.shape[0] \n",
    "num_test_samples = test_samples.shape[0] \n",
    "\n",
    "\n",
    "network_params =  ''.join([\n",
    "                  'time_steps={}-'.format(time_steps),          \n",
    "                  'latent_dim={}-'.format(z_dim),          \n",
    "                  'dataset={}'.format(data_sets[data_set_idx])])\n",
    "\n",
    "# Dir structure : /base_dir/network_params/run_xx/train_or_test/\n",
    "log_base_dir = os.path.join(log_root, network_params)\n",
    "\n",
    "# Check for previous runs\n",
    "if not os.path.isdir(log_base_dir):\n",
    "    os.makedirs(log_base_dir)\n",
    "\n",
    "previous_runs = os.listdir(log_base_dir)\n",
    "\n",
    "if len(previous_runs) == 0:\n",
    "    run_number = 1\n",
    "else:\n",
    "    run_number = max([int(str.split(s,'run_')[1]) for s in previous_runs if 'run' in s]) + 1\n",
    "\n",
    "log_dir = os.path.join(log_base_dir,'run_{0:02d}'.format(run_number))\n",
    "\n",
    "train_summary_writer = tf.summary.FileWriter(log_dir + '/train')\n",
    "valid_summary_writer = tf.summary.FileWriter(log_dir + '/valid')\n",
    "test_summary_writer = tf.summary.FileWriter(log_dir + '/test')\n",
    "model_save_path = log_dir + '/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(batch_size, x_in_dim, time_steps))\n",
    "\n",
    "# time_slices containts input x at time t across batches.\n",
    "x_in = time_steps * [None]\n",
    "x_out = time_steps * [None]\n",
    "h_enc = time_steps * [None]\n",
    "h_dec = (time_steps + 1) * [None]\n",
    "\n",
    "for t in range(time_steps):\n",
    "    x_in[t] = tf.squeeze(tf.slice(X,begin=[0,0,t],size=[-1,-1,1]),axis=2)\n",
    "\n",
    "###### Encoder network ###########\n",
    "with tf.variable_scope('encoder_rnn'):\n",
    "    cell_enc = tf.nn.rnn_cell.BasicRNNCell(num_hidden_units,activation=tf.nn.tanh)\n",
    "    h_enc[0] = tf.zeros([batch_size,num_hidden_units], dtype=tf.float32) # Initial state is 0\n",
    "\n",
    "    # h_t+1 = tanh(Wenc*h_t + Win*x_t+1 + b )\n",
    "    #Most basic RNN: output = new_state = act(W * input + U * state + B).\n",
    "    #https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/rnn_cell_impl.py\n",
    "    for t in range(time_steps-1):\n",
    "        _ , h_enc[t+1] = cell_enc(inputs=x_in[t+1], state=h_enc[t])\n",
    "\n",
    "\n",
    "mu_enc = tf.layers.dense(h_enc[-1], z_dim, activation=None, name='mu_enc')\n",
    "log_sigma_enc = tf.layers.dense(h_enc[-1], z_dim, activation=None, name='log_sigma_enc')\n",
    "\n",
    "###### Reparametrize ##############\n",
    "eps = tf.random_normal(tf.shape(log_sigma_enc))\n",
    "z = mu_enc + tf.exp(log_sigma_enc) * eps\n",
    "\n",
    "##### Decoder network ############\n",
    "with tf.variable_scope('decoder_rnn'):\n",
    "    W_out = tf.get_variable('W_out',shape=[num_hidden_units, x_in_dim])\n",
    "    b_out = tf.get_variable('b_out',shape=[x_in_dim])\n",
    "    \n",
    "    cell_dec = tf.nn.rnn_cell.BasicRNNCell(num_hidden_units,activation=tf.nn.tanh)\n",
    "    h_dec[0] = tf.layers.dense(z, num_hidden_units, activation=tf.nn.tanh)\n",
    "    \n",
    "    for t in range(time_steps):\n",
    "        x_out[t] = tf.nn.sigmoid(tf.matmul(h_dec[t], W_out) + b_out)\n",
    "        if t < time_steps - 1:\n",
    "            _, h_dec[t+1] = cell_dec(inputs=x_out[t], state=h_dec[t])\n",
    "\n",
    "##### Loss #####################\n",
    "with tf.variable_scope('loss'):\n",
    "    # Latent loss: -KL[q(z|x)|p(z)]\n",
    "    with tf.variable_scope('latent_loss'):\n",
    "        sigma_sq_enc = tf.square(tf.exp(log_sigma_enc))\n",
    "        latent_loss = -.5 * tf.reduce_mean(tf.reduce_sum((1 + tf.log(1e-10 + sigma_sq_enc)) - tf.square(mu_enc) - sigma_sq_enc, axis=1),axis=0)\n",
    "        latent_loss_summ = tf.summary.scalar('latent_loss',latent_loss)\n",
    "        \n",
    "    # Reconstruction Loss: log(p(x|z))    \n",
    "    with tf.variable_scope('recon_loss'):    \n",
    "        for i in range(time_steps):\n",
    "            if i == 0:\n",
    "                recon_loss_ = x_in[i] * tf.log(1e-10 + x_out[i]) + (1 - x_in[i]) * tf.log(1e-10+1-x_out[i])\n",
    "            else:\n",
    "                recon_loss_ += x_in[i] * tf.log(1e-10 + x_out[i]) + (1 - x_in[i]) * tf.log(1e-10+1-x_out[i])\n",
    "            \n",
    "        #collapse the loss, mean across a sample across all x_dim and time points, mean over batches\n",
    "        recon_loss = -tf.reduce_mean(tf.reduce_mean(recon_loss_/(time_steps),axis=1),axis=0)\n",
    "\n",
    "            \n",
    "    recon_loss_summ = tf.summary.scalar('recon_loss', recon_loss)\n",
    "                \n",
    "    with tf.variable_scope('total_loss'):\n",
    "        total_loss = latent_loss + recon_loss\n",
    "    \n",
    "    total_loss_summ = tf.summary.scalar('total_loss', total_loss)\n",
    "\n",
    "global_step = tf.Variable(0,name='global_step') \n",
    "\n",
    "#learning_rate = tf.train.exponential_decay(initial_learning_rate, epoch_num, num_epochs, decay_rate, staircase=False)\n",
    "learning_rate = tf.Variable(learning_rate_1,name='learning_rate')\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta_1, beta2=beta_2).minimize(total_loss,global_step=global_step)    \n",
    "scalar_summaries = tf.summary.merge([latent_loss_summ, recon_loss_summ, total_loss_summ])\n",
    "#image_summaries = tf.summary.merge()\n",
    "\n",
    "train_summary_writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 0.8590391905612598\n",
      "Average latent loss epoch 0: 0.4528458653927938\n",
      "Learning Rate 1.9999999494757503e-05\n",
      "Average loss epoch 1: 0.28704685918544526\n",
      "Average latent loss epoch 1: 0.08920862736631202\n",
      "Learning Rate 1.9999999494757503e-05\n",
      "Average loss epoch 2: 0.217303343559509\n",
      "Average latent loss epoch 2: 0.04351922950481987\n",
      "Learning Rate 1.9999999494757503e-05\n",
      "Average loss epoch 3: 0.1963820708532856\n",
      "Average latent loss epoch 3: 0.028095888764890907\n",
      "Learning Rate 1.9999999494757503e-05\n",
      "Average loss epoch 4: 0.18544180112886646\n",
      "Average latent loss epoch 4: 0.020533938228777828\n",
      "Learning Rate 1.9999999494757503e-05\n",
      "Average loss epoch 5: 0.17839438179040065\n",
      "Average latent loss epoch 5: 0.014745754797913168\n",
      "Learning Rate 9.999999747378752e-06\n",
      "Average loss epoch 6: 0.17459938998363878\n",
      "Average latent loss epoch 6: 0.012526423015448873\n",
      "Learning Rate 9.999999747378752e-06\n",
      "Average loss epoch 7: 0.17159649151373127\n",
      "Average latent loss epoch 7: 0.010843540420628165\n",
      "Learning Rate 9.999999747378752e-06\n",
      "Average loss epoch 8: 0.16871980601521933\n",
      "Average latent loss epoch 8: 0.009345700052145817\n",
      "Learning Rate 9.999999747378752e-06\n",
      "Average loss epoch 9: 0.16624080393140175\n",
      "Average latent loss epoch 9: 0.008388594732743148\n",
      "Learning Rate 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "num_batches = int(num_train_samples/batch_size)\n",
    "global_step_op = tf.train.get_global_step()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.\n",
    "        epoch_latent_loss = 0.\n",
    "        for batch in range(num_batches):\n",
    "            batch_num = sess.run(global_step_op)\n",
    "            \n",
    "            if epoch < num_epochs_to_diff_learn_rate:\n",
    "                curr_learning_rate = learning_rate_1\n",
    "            else:\n",
    "                curr_learning_rate = learning_rate_2\n",
    "            \n",
    "            #_ , loss, scalar_train_summaries, x_out_, x_in_,learning_rate_,latent_loss_ = \\\n",
    "            #sess.run([train_step, total_loss, scalar_summaries, x_out, x_in,learning_rate, latent_loss],feed_dict={X: feed_dict(batch_size,'train'), learning_rate: curr_learning_rate})\n",
    "            \n",
    "            _ , loss, scalar_train_summaries, learning_rate_, latent_loss_ = \\\n",
    "            sess.run([train_step, total_loss, scalar_summaries, learning_rate, latent_loss],feed_dict={X: feed_dict(batch_size,'train'), learning_rate: curr_learning_rate})\n",
    "            \n",
    "            # Check for NaN\n",
    "            if np.isnan(loss):\n",
    "                sys.exit(\"Loss during training at epoch: {}\".format(epoch))\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            epoch_latent_loss += latent_loss_\n",
    "            \n",
    "        print('Average loss epoch {0}: {1}'.format(epoch, epoch_loss/num_batches)) \n",
    "        print('Average latent loss epoch {0}: {1}'.format(epoch, epoch_latent_loss/num_batches)) \n",
    "        print('Learning Rate {}'.format(learning_rate_))\n",
    "        \n",
    "        # Write train summaries once a epoch\n",
    "        scalar_train_summaries = sess.run(scalar_summaries,feed_dict={X: feed_dict(batch_size,'train')})\n",
    "        train_summary_writer.add_summary(scalar_train_summaries, global_step=batch_num)\n",
    "        \n",
    "        # Write validation summaries\n",
    "        scalar_valid_summaries = sess.run(scalar_summaries,feed_dict={X: feed_dict(batch_size,'valid')})\n",
    "        valid_summary_writer.add_summary(scalar_valid_summaries, global_step=batch_num)\n",
    "        \n",
    "        # Write test summaries\n",
    "        scalar_test_summaries = sess.run(scalar_summaries,feed_dict={X: feed_dict(batch_size,'test')})\n",
    "        test_summary_writer.add_summary(scalar_test_summaries, global_step=batch_num)\n",
    "        \n",
    "        # Save the models\n",
    "        if epoch % num_epochs_to_save_model == 0:\n",
    "            save_path = saver.save(sess, model_save_path + '/epoch_{}.ckpt'.format(epoch))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_x_io(x, samp_num):\n",
    "    x_arr = np.asarray(x)\n",
    "    plt.imshow(x_arr[:,samp_num,:].T)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x_io(x_in_, 0)\n",
    "plot_x_io(x_out_, 0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
