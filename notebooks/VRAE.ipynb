{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from jupyterthemes import jtplot\n",
    "from IPython.core.debugger import Tracer\n",
    "jtplot.style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def clip_roll(piano_roll, time_steps=50):\n",
    "    samples = []\n",
    "    num_samples = int(piano_roll.shape[1] / time_steps)\n",
    "    for i in range(num_samples):\n",
    "        start_idx = time_steps*i\n",
    "        end_idx = (time_steps*(i+1))\n",
    "        samples.append(piano_roll[:,start_idx:end_idx])\n",
    "    return samples   \n",
    "\n",
    "def create_samples(load_root, time_steps=50, verbose=False):\n",
    "    samples = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(load_root):\n",
    "        for file in filenames:\n",
    "            if file.endswith('.npy'):\n",
    "                load_filepath = os.path.join(dirpath,file)\n",
    "                if verbose:\n",
    "                    print(load_filepath)\n",
    "                piano_roll = np.load(load_filepath).T\n",
    "                samples = samples + clip_roll(piano_roll,time_steps=time_steps)\n",
    "\n",
    "    return np.stack(samples)\n",
    "\n",
    "def feed_dict(batch_size):\n",
    "    indeces = np.random.randint(num_train_samples, size=batch_size)\n",
    "    return {X: np.take(samples, indeces, axis=0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X \\in {0,1}^{batch_size, dim_x, time_steps}\n",
    "time_steps = 10 #TEST\n",
    "x_in_dim = 88\n",
    "z_dim = 20 #TEST\n",
    "num_hidden_units = 500\n",
    "batch_size = 2#100 #TEST\n",
    "#learning_rate = 5*1e-6\n",
    "beta_1 = 0.05\n",
    "beta_2 = 0.001\n",
    "num_epochs = 400\n",
    "starter_learning_rate = 1e-4\n",
    "decay_rate = .6\n",
    "\n",
    "load_root = './MIDI_Data_PianoRolls/Nottingham/train/'\n",
    "samples = create_samples(load_root, time_steps=time_steps, verbose=False)\n",
    "num_train_samples = 10 #TEST #samples.shape[0] #TEST only pick 20 samples and overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(batch_size, x_in_dim, time_steps))\n",
    "\n",
    "# time_slices containts input x at time t across batches.\n",
    "x_in = time_steps * [None]\n",
    "x_out = time_steps * [None]\n",
    "h_enc = time_steps * [None]\n",
    "h_dec = (time_steps + 1) * [None]\n",
    "\n",
    "for t in range(time_steps):\n",
    "    x_in[t] = tf.squeeze(tf.slice(X,begin=[0,0,t],size=[-1,-1,1]),axis=2)\n",
    "\n",
    "###### Encoder network ###########\n",
    "with tf.variable_scope('encoder_rnn'):\n",
    "    cell_enc = tf.nn.rnn_cell.BasicRNNCell(num_hidden_units,activation=tf.nn.tanh)\n",
    "    h_enc[0] = tf.zeros([batch_size,num_hidden_units], dtype=tf.float32) # Initial state is 0\n",
    "\n",
    "    # h_t+1 = tanh(Wenc*h_t + Win*x_t+1 + b )\n",
    "    #Most basic RNN: output = new_state = act(W * input + U * state + B).\n",
    "    #https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/rnn_cell_impl.py\n",
    "    for t in range(time_steps-1):\n",
    "        _ , h_enc[t+1] = cell_enc(inputs=x_in[t+1], state=h_enc[t])\n",
    "\n",
    "\n",
    "mu_enc = tf.layers.dense(h_enc[-1], z_dim, activation=None, name='mu_enc')\n",
    "log_sigma_enc = tf.layers.dense(h_enc[-1], z_dim, activation=None, name='log_sigma_enc')\n",
    "\n",
    "###### Reparametrize ##############\n",
    "eps = tf.random_normal(tf.shape(log_sigma_enc))\n",
    "z = mu_enc + tf.exp(log_sigma_enc) * eps\n",
    "\n",
    "##### Decoder network ############\n",
    "with tf.variable_scope('decoder_rnn'):\n",
    "    W_out = tf.get_variable('W_out',shape=[num_hidden_units, x_in_dim])\n",
    "    b_out = tf.get_variable('b_out',shape=[x_in_dim])\n",
    "    \n",
    "    cell_dec = tf.nn.rnn_cell.BasicRNNCell(num_hidden_units,activation=tf.nn.tanh)\n",
    "    h_dec[0] = tf.layers.dense(z, num_hidden_units, activation=tf.nn.tanh)\n",
    "    \n",
    "    for t in range(time_steps):\n",
    "        x_out[t] = tf.nn.sigmoid(tf.matmul(h_dec[t], W_out) + b_out)\n",
    "        if t < time_steps - 1:\n",
    "            _, h_dec[t+1] = cell_dec(inputs=x_out[t], state=h_dec[t])\n",
    "\n",
    "##### Loss #####################\n",
    "with tf.variable_scope('loss'):\n",
    "    # Latent loss: -KL[q(z|x)|p(z)]\n",
    "    with tf.variable_scope('latent_loss'):\n",
    "        sigma_sq_enc = tf.square(tf.exp(log_sigma_enc))\n",
    "        latent_loss = -.5 * tf.reduce_mean(tf.reduce_sum((1 + tf.log(1e-10 + sigma_sq_enc)) - tf.square(mu_enc) - sigma_sq_enc, axis=1),axis=0)\n",
    "        latent_loss_summ = tf.summary.scalar('latent_loss',latent_loss)\n",
    "        \n",
    "    # Reconstruction Loss: log(p(x|z))    \n",
    "    with tf.variable_scope('recon_loss'):    \n",
    "        for i in range(time_steps):\n",
    "            if i == 0:\n",
    "                recon_loss_ = x_in[i] * tf.log(1e-10 + x_out[i]) + (1 - x_in[i]) * tf.log(1e-10+1-x_out[i])\n",
    "            else:\n",
    "                recon_loss_ += x_in[i] * tf.log(1e-10 + x_out[i]) + (1 - x_in[i]) * tf.log(1e-10+1-x_out[i])\n",
    "            \n",
    "        #collapse the loss, mean across a sample across all x_dim and time points, mean over batches\n",
    "        recon_loss = -tf.reduce_mean(tf.reduce_mean(recon_loss_/(time_steps),axis=1),axis=0)\n",
    "\n",
    "            \n",
    "    recon_loss_summ = tf.summary.scalar('recon_loss', recon_loss)\n",
    "                \n",
    "    with tf.variable_scope('total_loss'):\n",
    "        total_loss = latent_loss + recon_loss\n",
    "    \n",
    "    total_loss_summ = tf.summary.scalar('total_loss', total_loss)\n",
    "\n",
    "global_step = tf.Variable(0,name='global_step') \n",
    "epoch_num = tf.Variable(1, name='epoch_num', trainable=False, dtype=tf.int32)\n",
    "increment_epoch_num_op = tf.assign(epoch_num, epoch_num+1)\n",
    "\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, epoch_num, num_epochs, decay_rate, staircase=False)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta_1, beta2=beta_2).minimize(total_loss,global_step=global_step)    \n",
    "scalar_summaries = tf.summary.merge([latent_loss_summ, recon_loss_summ, total_loss_summ])\n",
    "#image_summaries = tf.summary.merge()\n",
    "\n",
    "train_summary_writer = tf.summary.FileWriter('./logs', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 2.0510775089263915\n",
      "Average latent loss epoch 0: 1.3754248142242431\n",
      "Learning Rate 9.987236990127712e-05\n",
      "Average loss epoch 1: 0.8320347428321838\n",
      "Average latent loss epoch 1: 0.24253873527050018\n",
      "Learning Rate 9.974491695174947e-05\n",
      "Average loss epoch 2: 0.6886101484298706\n",
      "Average latent loss epoch 2: 0.177860289812088\n",
      "Learning Rate 9.96176095213741e-05\n",
      "Average loss epoch 3: 0.5909326195716857\n",
      "Average latent loss epoch 3: 0.1512476921081543\n",
      "Learning Rate 9.949047671398148e-05\n",
      "Average loss epoch 4: 0.5765146017074585\n",
      "Average latent loss epoch 4: 0.19687959253787995\n",
      "Learning Rate 9.936349670169875e-05\n",
      "Average loss epoch 5: 0.48565603494644166\n",
      "Average latent loss epoch 5: 0.1591309756040573\n",
      "Learning Rate 9.923669131239876e-05\n",
      "Average loss epoch 6: 0.3961814343929291\n",
      "Average latent loss epoch 6: 0.10597045421600342\n",
      "Learning Rate 9.911003144225106e-05\n",
      "Average loss epoch 7: 0.35255649089813235\n",
      "Average latent loss epoch 7: 0.09430299401283264\n",
      "Learning Rate 9.898354619508609e-05\n",
      "Average loss epoch 8: 0.3416813135147095\n",
      "Average latent loss epoch 8: 0.10180546939373017\n",
      "Learning Rate 9.885722101898864e-05\n",
      "Average loss epoch 9: 0.277060729265213\n",
      "Average latent loss epoch 9: 0.06384643018245698\n",
      "Learning Rate 9.87310559139587e-05\n",
      "Average loss epoch 10: 0.2687740117311478\n",
      "Average latent loss epoch 10: 0.06631140410900116\n",
      "Learning Rate 9.860504360403866e-05\n",
      "Average loss epoch 11: 0.2755964457988739\n",
      "Average latent loss epoch 11: 0.08002623617649078\n",
      "Learning Rate 9.847920591710135e-05\n",
      "Average loss epoch 12: 0.2578244775533676\n",
      "Average latent loss epoch 12: 0.07409216165542602\n",
      "Learning Rate 9.835352102527395e-05\n",
      "Average loss epoch 13: 0.23702523112297058\n",
      "Average latent loss epoch 13: 0.06332197785377502\n",
      "Learning Rate 9.822799620451406e-05\n",
      "Average loss epoch 14: 0.2338648557662964\n",
      "Average latent loss epoch 14: 0.05974527895450592\n",
      "Learning Rate 9.810263145482168e-05\n",
      "Average loss epoch 15: 0.25061747133731843\n",
      "Average latent loss epoch 15: 0.08451313972473144\n",
      "Learning Rate 9.797742677619681e-05\n",
      "Average loss epoch 16: 0.2227581262588501\n",
      "Average latent loss epoch 16: 0.060552895069122314\n",
      "Learning Rate 9.785238216863945e-05\n",
      "Average loss epoch 17: 0.20134589970111846\n",
      "Average latent loss epoch 17: 0.04714557826519013\n",
      "Learning Rate 9.772749763214961e-05\n",
      "Average loss epoch 18: 0.20137366652488708\n",
      "Average latent loss epoch 18: 0.053931334614753725\n",
      "Learning Rate 9.760277316672727e-05\n",
      "Average loss epoch 19: 0.22421735525131226\n",
      "Average latent loss epoch 19: 0.07065328359603881\n",
      "Learning Rate 9.747821604833007e-05\n",
      "Average loss epoch 20: 0.20921655297279357\n",
      "Average latent loss epoch 20: 0.06660145819187165\n",
      "Learning Rate 9.735380444908515e-05\n",
      "Average loss epoch 21: 0.2082405984401703\n",
      "Average latent loss epoch 21: 0.052552783489227296\n",
      "Learning Rate 9.722955292090774e-05\n",
      "Average loss epoch 22: 0.187912517786026\n",
      "Average latent loss epoch 22: 0.041246405243873595\n",
      "Learning Rate 9.710546873975545e-05\n",
      "Average loss epoch 23: 0.19267513751983642\n",
      "Average latent loss epoch 23: 0.0555525153875351\n",
      "Learning Rate 9.698153735371307e-05\n",
      "Average loss epoch 24: 0.19871734380722045\n",
      "Average latent loss epoch 24: 0.05390827059745788\n",
      "Learning Rate 9.685776603873819e-05\n",
      "Average loss epoch 25: 0.19713030755519867\n",
      "Average latent loss epoch 25: 0.05414753258228302\n",
      "Learning Rate 9.673415479483083e-05\n",
      "Average loss epoch 26: 0.1834666132926941\n",
      "Average latent loss epoch 26: 0.043549925088882446\n",
      "Learning Rate 9.661068907007575e-05\n",
      "Average loss epoch 27: 0.19699669480323792\n",
      "Average latent loss epoch 27: 0.05776908993721008\n",
      "Learning Rate 9.648739796830341e-05\n",
      "Average loss epoch 28: 0.19762442409992217\n",
      "Average latent loss epoch 28: 0.04926384091377258\n",
      "Learning Rate 9.636425238568336e-05\n",
      "Average loss epoch 29: 0.17894653081893921\n",
      "Average latent loss epoch 29: 0.03814072012901306\n",
      "Learning Rate 9.624126687413082e-05\n",
      "Average loss epoch 30: 0.18641936480998994\n",
      "Average latent loss epoch 30: 0.04167478382587433\n",
      "Learning Rate 9.611844143364578e-05\n",
      "Average loss epoch 31: 0.17115446627140046\n",
      "Average latent loss epoch 31: 0.03299177885055542\n",
      "Learning Rate 9.599576878827065e-05\n",
      "Average loss epoch 32: 0.164982408285141\n",
      "Average latent loss epoch 32: 0.030525395274162294\n",
      "Learning Rate 9.587324893800542e-05\n",
      "Average loss epoch 33: 0.18021841943264008\n",
      "Average latent loss epoch 33: 0.04430671334266663\n",
      "Learning Rate 9.575089643476531e-05\n",
      "Average loss epoch 34: 0.15440378487110137\n",
      "Average latent loss epoch 34: 0.03706574141979217\n",
      "Learning Rate 9.56286967266351e-05\n",
      "Average loss epoch 35: 0.17235806584358215\n",
      "Average latent loss epoch 35: 0.03776455521583557\n",
      "Learning Rate 9.550664981361479e-05\n",
      "Average loss epoch 36: 0.16206132173538207\n",
      "Average latent loss epoch 36: 0.026082152128219606\n",
      "Learning Rate 9.538475569570437e-05\n",
      "Average loss epoch 37: 0.19585252106189727\n",
      "Average latent loss epoch 37: 0.05250989198684693\n",
      "Learning Rate 9.526302892481908e-05\n",
      "Average loss epoch 38: 0.19170626401901245\n",
      "Average latent loss epoch 38: 0.056759947538375856\n",
      "Learning Rate 9.514144767308608e-05\n",
      "Average loss epoch 39: 0.17661653459072113\n",
      "Average latent loss epoch 39: 0.042518329620361325\n",
      "Learning Rate 9.502001921646297e-05\n",
      "Average loss epoch 40: 0.16356572806835173\n",
      "Average latent loss epoch 40: 0.037873226404190066\n",
      "Learning Rate 9.489875083090737e-05\n",
      "Average loss epoch 41: 0.16831628680229188\n",
      "Average latent loss epoch 41: 0.039273566007614134\n",
      "Learning Rate 9.477763524046168e-05\n",
      "Average loss epoch 42: 0.16062342524528503\n",
      "Average latent loss epoch 42: 0.027059897780418396\n",
      "Learning Rate 9.465667244512588e-05\n",
      "Average loss epoch 43: 0.16141459345817566\n",
      "Average latent loss epoch 43: 0.028632578253746033\n",
      "Learning Rate 9.453586972085759e-05\n",
      "Average loss epoch 44: 0.1570887893438339\n",
      "Average latent loss epoch 44: 0.0295164555311203\n",
      "Learning Rate 9.44152197916992e-05\n",
      "Average loss epoch 45: 0.173202583193779\n",
      "Average latent loss epoch 45: 0.040391895174980166\n",
      "Learning Rate 9.429472265765071e-05\n",
      "Average loss epoch 46: 0.19208957850933076\n",
      "Average latent loss epoch 46: 0.04525720179080963\n",
      "Learning Rate 9.417437831871212e-05\n",
      "Average loss epoch 47: 0.1572258800268173\n",
      "Average latent loss epoch 47: 0.020671293139457703\n",
      "Learning Rate 9.405418677488342e-05\n",
      "Average loss epoch 48: 0.16374177038669585\n",
      "Average latent loss epoch 48: 0.028499853610992432\n",
      "Learning Rate 9.393415530212224e-05\n",
      "Average loss epoch 49: 0.14919605553150178\n",
      "Average latent loss epoch 49: 0.022034624218940736\n",
      "Learning Rate 9.381426934851333e-05\n",
      "Average loss epoch 50: 0.19665500819683074\n",
      "Average latent loss epoch 50: 0.06355744898319245\n",
      "Learning Rate 9.369453619001433e-05\n",
      "Average loss epoch 51: 0.19438937902450562\n",
      "Average latent loss epoch 51: 0.061560845375061034\n",
      "Learning Rate 9.357496310258284e-05\n",
      "Average loss epoch 52: 0.1524817317724228\n",
      "Average latent loss epoch 52: 0.021841716766357423\n",
      "Learning Rate 9.345553553430364e-05\n",
      "Average loss epoch 53: 0.17200478613376619\n",
      "Average latent loss epoch 53: 0.03929410576820373\n",
      "Learning Rate 9.333626076113433e-05\n",
      "Average loss epoch 54: 0.15068593472242356\n",
      "Average latent loss epoch 54: 0.03053860366344452\n",
      "Learning Rate 9.321714605903253e-05\n",
      "Average loss epoch 55: 0.1743570923805237\n",
      "Average latent loss epoch 55: 0.035770103335380554\n",
      "Learning Rate 9.30981696001254e-05\n",
      "Average loss epoch 56: 0.1580730140209198\n",
      "Average latent loss epoch 56: 0.03207722306251526\n",
      "Learning Rate 9.29793604882434e-05\n",
      "Average loss epoch 57: 0.15347467958927155\n",
      "Average latent loss epoch 57: 0.02910521924495697\n",
      "Learning Rate 9.286069689551368e-05\n",
      "Average loss epoch 58: 0.14274110794067382\n",
      "Average latent loss epoch 58: 0.018700739741325377\n",
      "Learning Rate 9.274217882193625e-05\n",
      "Average loss epoch 59: 0.15941337496042252\n",
      "Average latent loss epoch 59: 0.035060325264930726\n",
      "Learning Rate 9.262381354346871e-05\n",
      "Average loss epoch 60: 0.16068851351737976\n",
      "Average latent loss epoch 60: 0.03337005078792572\n",
      "Learning Rate 9.250560833606869e-05\n",
      "Average loss epoch 61: 0.16852337419986724\n",
      "Average latent loss epoch 61: 0.031211864948272706\n",
      "Learning Rate 9.238754864782095e-05\n",
      "Average loss epoch 62: 0.1501820296049118\n",
      "Average latent loss epoch 62: 0.019758591055870058\n",
      "Learning Rate 9.226964175468311e-05\n",
      "Average loss epoch 63: 0.15523422062397002\n",
      "Average latent loss epoch 63: 0.02638322114944458\n",
      "Learning Rate 9.215188038069755e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 64: 0.15122565925121306\n",
      "Average latent loss epoch 64: 0.021008649468421937\n",
      "Learning Rate 9.203427180182189e-05\n",
      "Average loss epoch 65: 0.14355795085430145\n",
      "Average latent loss epoch 65: 0.02187764346599579\n",
      "Learning Rate 9.191680874209851e-05\n",
      "Average loss epoch 66: 0.14490315318107605\n",
      "Average latent loss epoch 66: 0.01846420168876648\n",
      "Learning Rate 9.179949847748503e-05\n",
      "Average loss epoch 67: 0.14158032238483428\n",
      "Average latent loss epoch 67: 0.01946718990802765\n",
      "Learning Rate 9.168234100798145e-05\n",
      "Average loss epoch 68: 0.14691197574138642\n",
      "Average latent loss epoch 68: 0.02359512448310852\n",
      "Learning Rate 9.156532905763015e-05\n",
      "Average loss epoch 69: 0.15333107560873033\n",
      "Average latent loss epoch 69: 0.02956419885158539\n",
      "Learning Rate 9.144846990238875e-05\n",
      "Average loss epoch 70: 0.15082390904426574\n",
      "Average latent loss epoch 70: 0.023790210485458374\n",
      "Learning Rate 9.133176354225725e-05\n",
      "Average loss epoch 71: 0.14655233323574066\n",
      "Average latent loss epoch 71: 0.02331932485103607\n",
      "Learning Rate 9.121520270127803e-05\n",
      "Average loss epoch 72: 0.15274402499198914\n",
      "Average latent loss epoch 72: 0.023050162196159362\n",
      "Learning Rate 9.10987873794511e-05\n",
      "Average loss epoch 73: 0.14775171875953674\n",
      "Average latent loss epoch 73: 0.01868765652179718\n",
      "Learning Rate 9.098252485273406e-05\n",
      "Average loss epoch 74: 0.15308946073055268\n",
      "Average latent loss epoch 74: 0.02599523663520813\n",
      "Learning Rate 9.08664078451693e-05\n",
      "Average loss epoch 75: 0.14354922324419023\n",
      "Average latent loss epoch 75: 0.016983869671821594\n",
      "Learning Rate 9.075043635675684e-05\n",
      "Average loss epoch 76: 0.14705413281917573\n",
      "Average latent loss epoch 76: 0.017138445377349855\n",
      "Learning Rate 9.063461766345426e-05\n",
      "Average loss epoch 77: 0.1530812829732895\n",
      "Average latent loss epoch 77: 0.02394142746925354\n",
      "Learning Rate 9.051894448930398e-05\n",
      "Average loss epoch 78: 0.14344886243343352\n",
      "Average latent loss epoch 78: 0.020450249314308167\n",
      "Learning Rate 9.040342411026359e-05\n",
      "Average loss epoch 79: 0.1354433685541153\n",
      "Average latent loss epoch 79: 0.013953891396522523\n",
      "Learning Rate 9.028804197441787e-05\n",
      "Average loss epoch 80: 0.1330457642674446\n",
      "Average latent loss epoch 80: 0.015222492814064025\n",
      "Learning Rate 9.017281263368204e-05\n",
      "Average loss epoch 81: 0.14342662692070007\n",
      "Average latent loss epoch 81: 0.021078264713287352\n",
      "Learning Rate 9.00577288120985e-05\n",
      "Average loss epoch 82: 0.14850096106529237\n",
      "Average latent loss epoch 82: 0.023953831195831297\n",
      "Learning Rate 8.994279050966725e-05\n",
      "Average loss epoch 83: 0.13926542401313782\n",
      "Average latent loss epoch 83: 0.01636897325515747\n",
      "Learning Rate 8.982800500234589e-05\n",
      "Average loss epoch 84: 0.15270416140556337\n",
      "Average latent loss epoch 84: 0.021155908703804016\n",
      "Learning Rate 8.971336501417682e-05\n",
      "Average loss epoch 85: 0.1469831645488739\n",
      "Average latent loss epoch 85: 0.01992226541042328\n",
      "Learning Rate 8.959887054516003e-05\n",
      "Average loss epoch 86: 0.1447087287902832\n",
      "Average latent loss epoch 86: 0.021201106905937194\n",
      "Learning Rate 8.94845143193379e-05\n",
      "Average loss epoch 87: 0.1369004189968109\n",
      "Average latent loss epoch 87: 0.015070337057113647\n",
      "Learning Rate 8.937031088862568e-05\n",
      "Average loss epoch 88: 0.14186381101608275\n",
      "Average latent loss epoch 88: 0.02129577100276947\n",
      "Learning Rate 8.925625297706574e-05\n",
      "Average loss epoch 89: 0.13856509625911712\n",
      "Average latent loss epoch 89: 0.022879350185394286\n",
      "Learning Rate 8.914234058465809e-05\n",
      "Average loss epoch 90: 0.1399386078119278\n",
      "Average latent loss epoch 90: 0.01954635977745056\n",
      "Learning Rate 8.902857371140271e-05\n",
      "Average loss epoch 91: 0.14534585624933244\n",
      "Average latent loss epoch 91: 0.02796632647514343\n",
      "Learning Rate 8.891494508134201e-05\n",
      "Average loss epoch 92: 0.1470388203859329\n",
      "Average latent loss epoch 92: 0.021273666620254518\n",
      "Learning Rate 8.880146924639121e-05\n",
      "Average loss epoch 93: 0.12948067486286163\n",
      "Average latent loss epoch 93: 0.013465192914009095\n",
      "Learning Rate 8.868813893059269e-05\n",
      "Average loss epoch 94: 0.14482373744249344\n",
      "Average latent loss epoch 94: 0.016810494661331176\n",
      "Learning Rate 8.857494685798883e-05\n",
      "Average loss epoch 95: 0.12988977283239364\n",
      "Average latent loss epoch 95: 0.014593613147735596\n",
      "Learning Rate 8.846190030453727e-05\n",
      "Average loss epoch 96: 0.12219904512166976\n",
      "Average latent loss epoch 96: 0.014952623844146728\n",
      "Learning Rate 8.83490065461956e-05\n",
      "Average loss epoch 97: 0.13245045393705368\n",
      "Average latent loss epoch 97: 0.015917059779167176\n",
      "Learning Rate 8.82362510310486e-05\n",
      "Average loss epoch 98: 0.14159104079008103\n",
      "Average latent loss epoch 98: 0.01718912720680237\n",
      "Learning Rate 8.812364103505388e-05\n",
      "Average loss epoch 99: 0.13993927240371704\n",
      "Average latent loss epoch 99: 0.021972024440765382\n",
      "Learning Rate 8.801117655821145e-05\n",
      "Average loss epoch 100: 0.13890711963176727\n",
      "Average latent loss epoch 100: 0.01985612213611603\n",
      "Learning Rate 8.789885032456368e-05\n",
      "Average loss epoch 101: 0.13688048124313354\n",
      "Average latent loss epoch 101: 0.019051295518875123\n",
      "Learning Rate 8.77866696100682e-05\n",
      "Average loss epoch 102: 0.13390953987836837\n",
      "Average latent loss epoch 102: 0.01725170612335205\n",
      "Learning Rate 8.767462713876739e-05\n",
      "Average loss epoch 103: 0.13285411298274993\n",
      "Average latent loss epoch 103: 0.013969847559928894\n",
      "Learning Rate 8.756273746257648e-05\n",
      "Average loss epoch 104: 0.14087457954883575\n",
      "Average latent loss epoch 104: 0.023510068655014038\n",
      "Learning Rate 8.745098602958024e-05\n",
      "Average loss epoch 105: 0.12115763127803802\n",
      "Average latent loss epoch 105: 0.011855980753898621\n",
      "Learning Rate 8.733937283977866e-05\n",
      "Average loss epoch 106: 0.12896038293838502\n",
      "Average latent loss epoch 106: 0.01643291413784027\n",
      "Learning Rate 8.722790516912937e-05\n",
      "Average loss epoch 107: 0.13353461176156997\n",
      "Average latent loss epoch 107: 0.013656005263328552\n",
      "Learning Rate 8.711658301763237e-05\n",
      "Average loss epoch 108: 0.13003642559051515\n",
      "Average latent loss epoch 108: 0.010216370224952698\n",
      "Learning Rate 8.700539910933003e-05\n",
      "Average loss epoch 109: 0.1245397835969925\n",
      "Average latent loss epoch 109: 0.014945638179779053\n",
      "Learning Rate 8.689436072017998e-05\n",
      "Average loss epoch 110: 0.13042332231998444\n",
      "Average latent loss epoch 110: 0.018131718039512634\n",
      "Learning Rate 8.678346057422459e-05\n",
      "Average loss epoch 111: 0.12547941952943803\n",
      "Average latent loss epoch 111: 0.011956876516342163\n",
      "Learning Rate 8.667270594742149e-05\n",
      "Average loss epoch 112: 0.11591188758611679\n",
      "Average latent loss epoch 112: 0.009813368320465088\n",
      "Learning Rate 8.656208956381306e-05\n",
      "Average loss epoch 113: 0.12327248454093934\n",
      "Average latent loss epoch 113: 0.014358809590339661\n",
      "Learning Rate 8.64516114233993e-05\n",
      "Average loss epoch 114: 0.13738556802272797\n",
      "Average latent loss epoch 114: 0.016080045700073244\n",
      "Learning Rate 8.634127880213782e-05\n",
      "Average loss epoch 115: 0.12797416597604752\n",
      "Average latent loss epoch 115: 0.016458746790885926\n",
      "Learning Rate 8.623108442407101e-05\n",
      "Average loss epoch 116: 0.12511698305606841\n",
      "Average latent loss epoch 116: 0.010829421877861022\n",
      "Learning Rate 8.612103556515649e-05\n",
      "Average loss epoch 117: 0.11536920368671418\n",
      "Average latent loss epoch 117: 0.008885654807090759\n",
      "Learning Rate 8.601112494943663e-05\n",
      "Average loss epoch 118: 0.13171071857213973\n",
      "Average latent loss epoch 118: 0.014206477999687194\n",
      "Learning Rate 8.590134530095384e-05\n",
      "Average loss epoch 119: 0.1263972818851471\n",
      "Average latent loss epoch 119: 0.016360545158386232\n",
      "Learning Rate 8.579171844758093e-05\n",
      "Average loss epoch 120: 0.13020695596933365\n",
      "Average latent loss epoch 120: 0.016897362470626832\n",
      "Learning Rate 8.56822298374027e-05\n",
      "Average loss epoch 121: 0.12238446474075318\n",
      "Average latent loss epoch 121: 0.012037169933319092\n",
      "Learning Rate 8.557287219446152e-05\n",
      "Average loss epoch 122: 0.1309890031814575\n",
      "Average latent loss epoch 122: 0.011638477444648743\n",
      "Learning Rate 8.546366734663025e-05\n",
      "Average loss epoch 123: 0.1306191310286522\n",
      "Average latent loss epoch 123: 0.018016704916954042\n",
      "Learning Rate 8.535458619007841e-05\n",
      "Average loss epoch 124: 0.1265538066625595\n",
      "Average latent loss epoch 124: 0.016443213820457457\n",
      "Learning Rate 8.524565782863647e-05\n",
      "Average loss epoch 125: 0.12017782926559448\n",
      "Average latent loss epoch 125: 0.009666672348976136\n",
      "Learning Rate 8.51368677103892e-05\n",
      "Average loss epoch 126: 0.12321158051490784\n",
      "Average latent loss epoch 126: 0.012046858668327332\n",
      "Learning Rate 8.502820855937898e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 127: 0.12930529415607453\n",
      "Average latent loss epoch 127: 0.013430482149124146\n",
      "Learning Rate 8.491968765156344e-05\n",
      "Average loss epoch 128: 0.12844892144203185\n",
      "Average latent loss epoch 128: 0.018895718455314636\n",
      "Learning Rate 8.481131226290017e-05\n",
      "Average loss epoch 129: 0.12184174656867981\n",
      "Average latent loss epoch 129: 0.00929492712020874\n",
      "Learning Rate 8.470306784147397e-05\n",
      "Average loss epoch 130: 0.11969386488199234\n",
      "Average latent loss epoch 130: 0.014370301365852356\n",
      "Learning Rate 8.459496893920004e-05\n",
      "Average loss epoch 131: 0.12058843970298767\n",
      "Average latent loss epoch 131: 0.009424114227294922\n",
      "Learning Rate 8.448700100416318e-05\n",
      "Average loss epoch 132: 0.12385477721691132\n",
      "Average latent loss epoch 132: 0.012587159872055054\n",
      "Learning Rate 8.437917858827859e-05\n",
      "Average loss epoch 133: 0.10995623916387558\n",
      "Average latent loss epoch 133: 0.011628809571266174\n",
      "Learning Rate 8.427148713963106e-05\n",
      "Average loss epoch 134: 0.10149878412485122\n",
      "Average latent loss epoch 134: 0.0099077045917511\n",
      "Learning Rate 8.416394121013582e-05\n",
      "Average loss epoch 135: 0.12174364924430847\n",
      "Average latent loss epoch 135: 0.014260971546173095\n",
      "Learning Rate 8.405652624787763e-05\n",
      "Average loss epoch 136: 0.12106463760137558\n",
      "Average latent loss epoch 136: 0.015055054426193237\n",
      "Learning Rate 8.394924952881411e-05\n",
      "Average loss epoch 137: 0.12587936818599701\n",
      "Average latent loss epoch 137: 0.012014287710189819\n",
      "Learning Rate 8.384211105294526e-05\n",
      "Average loss epoch 138: 0.12399077862501144\n",
      "Average latent loss epoch 138: 0.010819184780120849\n",
      "Learning Rate 8.373510354431346e-05\n",
      "Average loss epoch 139: 0.11706901341676712\n",
      "Average latent loss epoch 139: 0.012051495909690856\n",
      "Learning Rate 8.362823427887633e-05\n",
      "Average loss epoch 140: 0.11466617733240128\n",
      "Average latent loss epoch 140: 0.012840703129768372\n",
      "Learning Rate 8.352151053259149e-05\n",
      "Average loss epoch 141: 0.11428855955600739\n",
      "Average latent loss epoch 141: 0.008878722786903381\n",
      "Learning Rate 8.341491047758609e-05\n",
      "Average loss epoch 142: 0.12584326565265655\n",
      "Average latent loss epoch 142: 0.02175095081329346\n",
      "Learning Rate 8.330844866577536e-05\n",
      "Average loss epoch 143: 0.12131740152835846\n",
      "Average latent loss epoch 143: 0.010952484607696534\n",
      "Learning Rate 8.32021250971593e-05\n",
      "Average loss epoch 144: 0.11151770502328873\n",
      "Average latent loss epoch 144: 0.008027228713035583\n",
      "Learning Rate 8.30959397717379e-05\n",
      "Average loss epoch 145: 0.11417736411094666\n",
      "Average latent loss epoch 145: 0.009361499547958374\n",
      "Learning Rate 8.298989268951118e-05\n",
      "Average loss epoch 146: 0.10765401721000671\n",
      "Average latent loss epoch 146: 0.011519360542297363\n",
      "Learning Rate 8.288397657452151e-05\n",
      "Average loss epoch 147: 0.11695164293050767\n",
      "Average latent loss epoch 147: 0.011058685183525086\n",
      "Learning Rate 8.277819870272651e-05\n",
      "Average loss epoch 148: 0.1108826756477356\n",
      "Average latent loss epoch 148: 0.01013081967830658\n",
      "Learning Rate 8.267255179816857e-05\n",
      "Average loss epoch 149: 0.11672589629888534\n",
      "Average latent loss epoch 149: 0.008728814125061036\n",
      "Learning Rate 8.25670431368053e-05\n",
      "Average loss epoch 150: 0.11170569956302642\n",
      "Average latent loss epoch 150: 0.0080795019865036\n",
      "Learning Rate 8.246166544267908e-05\n",
      "Average loss epoch 151: 0.11023944914340973\n",
      "Average latent loss epoch 151: 0.013449814915657044\n",
      "Learning Rate 8.235641871578991e-05\n",
      "Average loss epoch 152: 0.10992721766233444\n",
      "Average latent loss epoch 152: 0.010929524898529053\n",
      "Learning Rate 8.225131750805303e-05\n",
      "Average loss epoch 153: 0.1104020431637764\n",
      "Average latent loss epoch 153: 0.008462342619895934\n",
      "Learning Rate 8.214634726755321e-05\n",
      "Average loss epoch 154: 0.11226103156805038\n",
      "Average latent loss epoch 154: 0.0093904048204422\n",
      "Learning Rate 8.204150799429044e-05\n",
      "Average loss epoch 155: 0.10954738110303879\n",
      "Average latent loss epoch 155: 0.007954838871955871\n",
      "Learning Rate 8.193679968826473e-05\n",
      "Average loss epoch 156: 0.11211342960596085\n",
      "Average latent loss epoch 156: 0.009453767538070678\n",
      "Learning Rate 8.183222234947607e-05\n",
      "Average loss epoch 157: 0.11446647346019745\n",
      "Average latent loss epoch 157: 0.008953562378883362\n",
      "Learning Rate 8.17277905298397e-05\n",
      "Average loss epoch 158: 0.11762618869543076\n",
      "Average latent loss epoch 158: 0.011482509970664977\n",
      "Learning Rate 8.162348240148276e-05\n",
      "Average loss epoch 159: 0.11354120969772338\n",
      "Average latent loss epoch 159: 0.011442598700523377\n",
      "Learning Rate 8.15193125163205e-05\n",
      "Average loss epoch 160: 0.10321364551782608\n",
      "Average latent loss epoch 160: 0.00954614281654358\n",
      "Learning Rate 8.141526632243767e-05\n",
      "Average loss epoch 161: 0.11833871901035309\n",
      "Average latent loss epoch 161: 0.010948577523231506\n",
      "Learning Rate 8.131136564770713e-05\n",
      "Average loss epoch 162: 0.10937891751527787\n",
      "Average latent loss epoch 162: 0.010517069697380066\n",
      "Learning Rate 8.120758866425604e-05\n",
      "Average loss epoch 163: 0.11039807796478271\n",
      "Average latent loss epoch 163: 0.011595234274864197\n",
      "Learning Rate 8.110394992399961e-05\n",
      "Average loss epoch 164: 0.10276664644479752\n",
      "Average latent loss epoch 164: 0.006061080098152161\n",
      "Learning Rate 8.100044215098023e-05\n",
      "Average loss epoch 165: 0.10449295043945313\n",
      "Average latent loss epoch 165: 0.009596493840217591\n",
      "Learning Rate 8.089706534519792e-05\n",
      "Average loss epoch 166: 0.10376267433166504\n",
      "Average latent loss epoch 166: 0.0072688966989517215\n",
      "Learning Rate 8.079381950665265e-05\n",
      "Average loss epoch 167: 0.10682196468114853\n",
      "Average latent loss epoch 167: 0.008743953704833985\n",
      "Learning Rate 8.069071191130206e-05\n",
      "Average loss epoch 168: 0.10421740412712097\n",
      "Average latent loss epoch 168: 0.011342740058898926\n",
      "Learning Rate 8.058772800723091e-05\n",
      "Average loss epoch 169: 0.10636933445930481\n",
      "Average latent loss epoch 169: 0.009584259986877442\n",
      "Learning Rate 8.048487507039681e-05\n",
      "Average loss epoch 170: 0.09696233868598939\n",
      "Average latent loss epoch 170: 0.005171328783035278\n",
      "Learning Rate 8.038215310079977e-05\n",
      "Average loss epoch 171: 0.10830957442522049\n",
      "Average latent loss epoch 171: 0.009709772467613221\n",
      "Learning Rate 8.02795693743974e-05\n",
      "Average loss epoch 172: 0.10929611772298813\n",
      "Average latent loss epoch 172: 0.00986887514591217\n",
      "Learning Rate 8.017711661523208e-05\n",
      "Average loss epoch 173: 0.11115531474351883\n",
      "Average latent loss epoch 173: 0.008398059010505676\n",
      "Learning Rate 8.00747875473462e-05\n",
      "Average loss epoch 174: 0.10359358042478561\n",
      "Average latent loss epoch 174: 0.008596426248550415\n",
      "Learning Rate 7.997258944669738e-05\n",
      "Average loss epoch 175: 0.10026846230030059\n",
      "Average latent loss epoch 175: 0.007015225291252136\n",
      "Learning Rate 7.987052958924323e-05\n",
      "Average loss epoch 176: 0.11101341247558594\n",
      "Average latent loss epoch 176: 0.010269615054130554\n",
      "Learning Rate 7.976859342306852e-05\n",
      "Average loss epoch 177: 0.11208882033824921\n",
      "Average latent loss epoch 177: 0.009246313571929931\n",
      "Learning Rate 7.966678822413087e-05\n",
      "Average loss epoch 178: 0.11542950868606568\n",
      "Average latent loss epoch 178: 0.00984155833721161\n",
      "Learning Rate 7.956511399243027e-05\n",
      "Average loss epoch 179: 0.1100526437163353\n",
      "Average latent loss epoch 179: 0.011761161684989929\n",
      "Learning Rate 7.946357072796673e-05\n",
      "Average loss epoch 180: 0.10327065140008926\n",
      "Average latent loss epoch 180: 0.009136644005775452\n",
      "Learning Rate 7.936215115478262e-05\n",
      "Average loss epoch 181: 0.10462275743484498\n",
      "Average latent loss epoch 181: 0.008597427606582641\n",
      "Learning Rate 7.926086254883558e-05\n",
      "Average loss epoch 182: 0.10534836798906326\n",
      "Average latent loss epoch 182: 0.00842832624912262\n",
      "Learning Rate 7.91597121860832e-05\n",
      "Average loss epoch 183: 0.10335654020309448\n",
      "Average latent loss epoch 183: 0.007124081254005432\n",
      "Learning Rate 7.905867823865265e-05\n",
      "Average loss epoch 184: 0.09879992753267289\n",
      "Average latent loss epoch 184: 0.009213069081306457\n",
      "Learning Rate 7.895778253441676e-05\n",
      "Average loss epoch 185: 0.10638342797756195\n",
      "Average latent loss epoch 185: 0.0073951900005340574\n",
      "Learning Rate 7.885701779741794e-05\n",
      "Average loss epoch 186: 0.10572303384542465\n",
      "Average latent loss epoch 186: 0.008602878451347351\n",
      "Learning Rate 7.875636947574094e-05\n",
      "Average loss epoch 187: 0.10130155831575394\n",
      "Average latent loss epoch 187: 0.008666020631790162\n",
      "Learning Rate 7.865585939725861e-05\n",
      "Average loss epoch 188: 0.10319806188344956\n",
      "Average latent loss epoch 188: 0.011768901348114013\n",
      "Learning Rate 7.855547301005572e-05\n",
      "Average loss epoch 189: 0.10471708625555039\n",
      "Average latent loss epoch 189: 0.009073764085769653\n",
      "Learning Rate 7.845521759008989e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 190: 0.09930073320865632\n",
      "Average latent loss epoch 190: 0.008691501617431641\n",
      "Learning Rate 7.835509313736111e-05\n",
      "Average loss epoch 191: 0.09532948732376098\n",
      "Average latent loss epoch 191: 0.006566011905670166\n",
      "Learning Rate 7.825509237591177e-05\n",
      "Average loss epoch 192: 0.09465941786766052\n",
      "Average latent loss epoch 192: 0.006733420491218567\n",
      "Learning Rate 7.815521530574188e-05\n",
      "Average loss epoch 193: 0.09571240693330765\n",
      "Average latent loss epoch 193: 0.00976661741733551\n",
      "Learning Rate 7.805546920280904e-05\n",
      "Average loss epoch 194: 0.0959065705537796\n",
      "Average latent loss epoch 194: 0.010163754224777222\n",
      "Learning Rate 7.795585406711325e-05\n",
      "Average loss epoch 195: 0.09567812085151672\n",
      "Average latent loss epoch 195: 0.009456098079681396\n",
      "Learning Rate 7.78563626226969e-05\n",
      "Average loss epoch 196: 0.09730214178562165\n",
      "Average latent loss epoch 196: 0.00655631422996521\n",
      "Learning Rate 7.775700214551762e-05\n",
      "Average loss epoch 197: 0.08483879566192627\n",
      "Average latent loss epoch 197: 0.005772259831428528\n",
      "Learning Rate 7.765776535961777e-05\n",
      "Average loss epoch 198: 0.1048788920044899\n",
      "Average latent loss epoch 198: 0.010134479403495789\n",
      "Learning Rate 7.755865226499736e-05\n",
      "Average loss epoch 199: 0.09038514643907547\n",
      "Average latent loss epoch 199: 0.006932941079139709\n",
      "Learning Rate 7.745967013761401e-05\n",
      "Average loss epoch 200: 0.1001336395740509\n",
      "Average latent loss epoch 200: 0.005537870526313782\n",
      "Learning Rate 7.736080442555249e-05\n",
      "Average loss epoch 201: 0.10108523964881896\n",
      "Average latent loss epoch 201: 0.007713055610656739\n",
      "Learning Rate 7.726207695668563e-05\n",
      "Average loss epoch 202: 0.08937114924192428\n",
      "Average latent loss epoch 202: 0.00545903742313385\n",
      "Learning Rate 7.716347317909822e-05\n",
      "Average loss epoch 203: 0.08086106777191163\n",
      "Average latent loss epoch 203: 0.004629960656166077\n",
      "Learning Rate 7.706498581683263e-05\n",
      "Average loss epoch 204: 0.09538150429725648\n",
      "Average latent loss epoch 204: 0.009126341342926026\n",
      "Learning Rate 7.696663669776171e-05\n",
      "Average loss epoch 205: 0.09226863533258438\n",
      "Average latent loss epoch 205: 0.007284677028656006\n",
      "Learning Rate 7.686840399401262e-05\n",
      "Average loss epoch 206: 0.09804862886667251\n",
      "Average latent loss epoch 206: 0.00747273862361908\n",
      "Learning Rate 7.677030225750059e-05\n",
      "Average loss epoch 207: 0.09573600888252258\n",
      "Average latent loss epoch 207: 0.006957978010177612\n",
      "Learning Rate 7.6672324212268e-05\n",
      "Average loss epoch 208: 0.09509700536727905\n",
      "Average latent loss epoch 208: 0.007494774460792541\n",
      "Learning Rate 7.657446985831484e-05\n",
      "Average loss epoch 209: 0.09637085348367691\n",
      "Average latent loss epoch 209: 0.00567852258682251\n",
      "Learning Rate 7.647674647159874e-05\n",
      "Average loss epoch 210: 0.0918225735425949\n",
      "Average latent loss epoch 210: 0.006341570615768432\n",
      "Learning Rate 7.637913950020447e-05\n",
      "Average loss epoch 211: 0.09030445218086243\n",
      "Average latent loss epoch 211: 0.005288225412368774\n",
      "Learning Rate 7.628166349604726e-05\n",
      "Average loss epoch 212: 0.09015725255012512\n",
      "Average latent loss epoch 212: 0.006910583376884461\n",
      "Learning Rate 7.618430390721187e-05\n",
      "Average loss epoch 213: 0.1039426565170288\n",
      "Average latent loss epoch 213: 0.010714367032051086\n",
      "Learning Rate 7.608708256157115e-05\n",
      "Average loss epoch 214: 0.09334429949522019\n",
      "Average latent loss epoch 214: 0.007251414656639099\n",
      "Learning Rate 7.598997035529464e-05\n",
      "Average loss epoch 215: 0.09394041001796723\n",
      "Average latent loss epoch 215: 0.0068757891654968265\n",
      "Learning Rate 7.58929891162552e-05\n",
      "Average loss epoch 216: 0.09741570502519607\n",
      "Average latent loss epoch 216: 0.007440489530563354\n",
      "Learning Rate 7.579613156849518e-05\n",
      "Average loss epoch 217: 0.0929195299744606\n",
      "Average latent loss epoch 217: 0.00539313554763794\n",
      "Learning Rate 7.569939771201462e-05\n",
      "Average loss epoch 218: 0.08489202857017517\n",
      "Average latent loss epoch 218: 0.007580891251564026\n",
      "Learning Rate 7.560278754681349e-05\n",
      "Average loss epoch 219: 0.10229013562202453\n",
      "Average latent loss epoch 219: 0.008852627873420716\n",
      "Learning Rate 7.55063010728918e-05\n",
      "Average loss epoch 220: 0.10160221755504609\n",
      "Average latent loss epoch 220: 0.010252168774604798\n",
      "Learning Rate 7.540993829024956e-05\n",
      "Average loss epoch 221: 0.0990613728761673\n",
      "Average latent loss epoch 221: 0.005810260772705078\n",
      "Learning Rate 7.531369192292914e-05\n",
      "Average loss epoch 222: 0.09375727474689484\n",
      "Average latent loss epoch 222: 0.00658467710018158\n",
      "Learning Rate 7.521757652284577e-05\n",
      "Average loss epoch 223: 0.08563154935836792\n",
      "Average latent loss epoch 223: 0.006194394826889038\n",
      "Learning Rate 7.512157753808424e-05\n",
      "Average loss epoch 224: 0.09861090630292893\n",
      "Average latent loss epoch 224: 0.006691381335258484\n",
      "Learning Rate 7.502570224460214e-05\n",
      "Average loss epoch 225: 0.09077514111995696\n",
      "Average latent loss epoch 225: 0.007180061936378479\n",
      "Learning Rate 7.492995064239949e-05\n",
      "Average loss epoch 226: 0.09220728129148484\n",
      "Average latent loss epoch 226: 0.006379804015159607\n",
      "Learning Rate 7.483432273147628e-05\n",
      "Average loss epoch 227: 0.09145191162824631\n",
      "Average latent loss epoch 227: 0.005018496513366699\n",
      "Learning Rate 7.47388185118325e-05\n",
      "Average loss epoch 228: 0.09353175163269042\n",
      "Average latent loss epoch 228: 0.007259178161621094\n",
      "Learning Rate 7.464343070751056e-05\n",
      "Average loss epoch 229: 0.09106829464435577\n",
      "Average latent loss epoch 229: 0.0044443279504776\n",
      "Learning Rate 7.454816659446806e-05\n",
      "Average loss epoch 230: 0.09513276517391205\n",
      "Average latent loss epoch 230: 0.00837511122226715\n",
      "Learning Rate 7.4453026172705e-05\n",
      "Average loss epoch 231: 0.09679696857929229\n",
      "Average latent loss epoch 231: 0.008055466413497924\n",
      "Learning Rate 7.435800216626376e-05\n",
      "Average loss epoch 232: 0.08644711971282959\n",
      "Average latent loss epoch 232: 0.00477052628993988\n",
      "Learning Rate 7.426310912705958e-05\n",
      "Average loss epoch 233: 0.09397508651018142\n",
      "Average latent loss epoch 233: 0.006702849268913269\n",
      "Learning Rate 7.416832522721961e-05\n",
      "Average loss epoch 234: 0.09602106511592864\n",
      "Average latent loss epoch 234: 0.0068181902170181274\n",
      "Learning Rate 7.40736722946167e-05\n",
      "Average loss epoch 235: 0.09583517760038376\n",
      "Average latent loss epoch 235: 0.006336206197738647\n",
      "Learning Rate 7.3979128501378e-05\n",
      "Average loss epoch 236: 0.08554409593343734\n",
      "Average latent loss epoch 236: 0.0040493935346603395\n",
      "Learning Rate 7.388471567537636e-05\n",
      "Average loss epoch 237: 0.09164079874753953\n",
      "Average latent loss epoch 237: 0.006394624710083008\n",
      "Learning Rate 7.379042654065415e-05\n",
      "Average loss epoch 238: 0.08676774799823761\n",
      "Average latent loss epoch 238: 0.006220185756683349\n",
      "Learning Rate 7.369624654529616e-05\n",
      "Average loss epoch 239: 0.09621718525886536\n",
      "Average latent loss epoch 239: 0.00645761787891388\n",
      "Learning Rate 7.360219024121761e-05\n",
      "Average loss epoch 240: 0.0849437728524208\n",
      "Average latent loss epoch 240: 0.005771049857139587\n",
      "Learning Rate 7.35082576284185e-05\n",
      "Average loss epoch 241: 0.08887290507555008\n",
      "Average latent loss epoch 241: 0.005865070223808289\n",
      "Learning Rate 7.341444143094122e-05\n",
      "Average loss epoch 242: 0.0911232978105545\n",
      "Average latent loss epoch 242: 0.005647706985473633\n",
      "Learning Rate 7.332074164878577e-05\n",
      "Average loss epoch 243: 0.08744644969701768\n",
      "Average latent loss epoch 243: 0.004676458239555359\n",
      "Learning Rate 7.322717283386737e-05\n",
      "Average loss epoch 244: 0.08717691153287888\n",
      "Average latent loss epoch 244: 0.005450144410133362\n",
      "Learning Rate 7.31337204342708e-05\n",
      "Average loss epoch 245: 0.0873742625117302\n",
      "Average latent loss epoch 245: 0.0054066002368927\n",
      "Learning Rate 7.304037717403844e-05\n",
      "Average loss epoch 246: 0.08948034644126893\n",
      "Average latent loss epoch 246: 0.007071954011917114\n",
      "Learning Rate 7.294715760508552e-05\n",
      "Average loss epoch 247: 0.08813241720199586\n",
      "Average latent loss epoch 247: 0.00411374568939209\n",
      "Learning Rate 7.285406172741205e-05\n",
      "Average loss epoch 248: 0.0835189625620842\n",
      "Average latent loss epoch 248: 0.005588275194168091\n",
      "Learning Rate 7.27610822650604e-05\n",
      "Average loss epoch 249: 0.08788308501243591\n",
      "Average latent loss epoch 249: 0.005262604355812073\n",
      "Learning Rate 7.266821921803057e-05\n",
      "Average loss epoch 250: 0.08751199543476104\n",
      "Average latent loss epoch 250: 0.005951353907585144\n",
      "Learning Rate 7.257547986228019e-05\n",
      "Average loss epoch 251: 0.07913645058870315\n",
      "Average latent loss epoch 251: 0.004267767071723938\n",
      "Learning Rate 7.248284964589402e-05\n",
      "Average loss epoch 252: 0.09019863605499268\n",
      "Average latent loss epoch 252: 0.004357990622520447\n",
      "Learning Rate 7.239034312078729e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 253: 0.08030912801623344\n",
      "Average latent loss epoch 253: 0.0067033559083938595\n",
      "Learning Rate 7.229796028696e-05\n",
      "Average loss epoch 254: 0.10060711950063705\n",
      "Average latent loss epoch 254: 0.008674317598342895\n",
      "Learning Rate 7.220569386845455e-05\n",
      "Average loss epoch 255: 0.08769882321357728\n",
      "Average latent loss epoch 255: 0.004904738068580628\n",
      "Learning Rate 7.21135365893133e-05\n",
      "Average loss epoch 256: 0.08127294480800629\n",
      "Average latent loss epoch 256: 0.005068260431289673\n",
      "Learning Rate 7.202150300145149e-05\n",
      "Average loss epoch 257: 0.09476274996995926\n",
      "Average latent loss epoch 257: 0.005749252438545227\n",
      "Learning Rate 7.192958582891151e-05\n",
      "Average loss epoch 258: 0.0809779554605484\n",
      "Average latent loss epoch 258: 0.0038651436567306518\n",
      "Learning Rate 7.183778507169336e-05\n",
      "Average loss epoch 259: 0.08526166677474975\n",
      "Average latent loss epoch 259: 0.006369754672050476\n",
      "Learning Rate 7.174610072979704e-05\n",
      "Average loss epoch 260: 0.08402476757764817\n",
      "Average latent loss epoch 260: 0.004746174812316895\n",
      "Learning Rate 7.165454007918015e-05\n",
      "Average loss epoch 261: 0.08304454982280732\n",
      "Average latent loss epoch 261: 0.005486926436424256\n",
      "Learning Rate 7.156308856792748e-05\n",
      "Average loss epoch 262: 0.09248002469539643\n",
      "Average latent loss epoch 262: 0.005844128131866455\n",
      "Learning Rate 7.147175347199664e-05\n",
      "Average loss epoch 263: 0.08384989202022552\n",
      "Average latent loss epoch 263: 0.003702053427696228\n",
      "Learning Rate 7.138053479138762e-05\n",
      "Average loss epoch 264: 0.08550098538398743\n",
      "Average latent loss epoch 264: 0.005268222093582154\n",
      "Learning Rate 7.128943980205804e-05\n",
      "Average loss epoch 265: 0.09339838027954102\n",
      "Average latent loss epoch 265: 0.007213670015335083\n",
      "Learning Rate 7.119845395209268e-05\n",
      "Average loss epoch 266: 0.07739152312278748\n",
      "Average latent loss epoch 266: 0.004095378518104553\n",
      "Learning Rate 7.110759179340675e-05\n",
      "Average loss epoch 267: 0.0779984638094902\n",
      "Average latent loss epoch 267: 0.0038958460092544554\n",
      "Learning Rate 7.101683877408504e-05\n",
      "Average loss epoch 268: 0.08642838895320892\n",
      "Average latent loss epoch 268: 0.004365113377571106\n",
      "Learning Rate 7.092620217008516e-05\n",
      "Average loss epoch 269: 0.08468610942363738\n",
      "Average latent loss epoch 269: 0.004818674921989441\n",
      "Learning Rate 7.08356819814071e-05\n",
      "Average loss epoch 270: 0.08312611430883407\n",
      "Average latent loss epoch 270: 0.0032294213771820067\n",
      "Learning Rate 7.074527820805088e-05\n",
      "Average loss epoch 271: 0.08160958588123321\n",
      "Average latent loss epoch 271: 0.005218976736068725\n",
      "Learning Rate 7.065499085001647e-05\n",
      "Average loss epoch 272: 0.09348886609077453\n",
      "Average latent loss epoch 272: 0.009449073672294616\n",
      "Learning Rate 7.05648199073039e-05\n",
      "Average loss epoch 273: 0.08329770863056182\n",
      "Average latent loss epoch 273: 0.00550290048122406\n",
      "Learning Rate 7.047475810395554e-05\n",
      "Average loss epoch 274: 0.07360051423311234\n",
      "Average latent loss epoch 274: 0.003074917197227478\n",
      "Learning Rate 7.038481999188662e-05\n",
      "Average loss epoch 275: 0.08143066316843033\n",
      "Average latent loss epoch 275: 0.005264890193939209\n",
      "Learning Rate 7.029498374322429e-05\n",
      "Average loss epoch 276: 0.08437602519989014\n",
      "Average latent loss epoch 276: 0.005232161283493042\n",
      "Learning Rate 7.020527846179903e-05\n",
      "Average loss epoch 277: 0.0903852939605713\n",
      "Average latent loss epoch 277: 0.006037119030952454\n",
      "Learning Rate 7.011567504378036e-05\n",
      "Average loss epoch 278: 0.08483965545892716\n",
      "Average latent loss epoch 278: 0.005951127409934998\n",
      "Learning Rate 7.002619531704113e-05\n",
      "Average loss epoch 279: 0.08302427977323532\n",
      "Average latent loss epoch 279: 0.0033574849367141724\n",
      "Learning Rate 6.993682472966611e-05\n",
      "Average loss epoch 280: 0.09035863429307937\n",
      "Average latent loss epoch 280: 0.007153436541557312\n",
      "Learning Rate 6.984756328165531e-05\n",
      "Average loss epoch 281: 0.08102454394102096\n",
      "Average latent loss epoch 281: 0.004364034533500672\n",
      "Learning Rate 6.975841824896634e-05\n",
      "Average loss epoch 282: 0.07873051762580871\n",
      "Average latent loss epoch 282: 0.002852514386177063\n",
      "Learning Rate 6.96693969075568e-05\n",
      "Average loss epoch 283: 0.08116501718759536\n",
      "Average latent loss epoch 283: 0.0039467453956604\n",
      "Learning Rate 6.958047742955387e-05\n",
      "Average loss epoch 284: 0.0781828448176384\n",
      "Average latent loss epoch 284: 0.0044031858444213865\n",
      "Learning Rate 6.949167436687276e-05\n",
      "Average loss epoch 285: 0.08455384820699692\n",
      "Average latent loss epoch 285: 0.005345979332923889\n",
      "Learning Rate 6.940298771951348e-05\n",
      "Average loss epoch 286: 0.08711078017950058\n",
      "Average latent loss epoch 286: 0.0032921820878982545\n",
      "Learning Rate 6.931441021151841e-05\n",
      "Average loss epoch 287: 0.07872789651155472\n",
      "Average latent loss epoch 287: 0.005449482798576355\n",
      "Learning Rate 6.922594911884516e-05\n",
      "Average loss epoch 288: 0.0737259715795517\n",
      "Average latent loss epoch 288: 0.004737848043441772\n",
      "Learning Rate 6.913759716553614e-05\n",
      "Average loss epoch 289: 0.08583770990371704\n",
      "Average latent loss epoch 289: 0.006221270561218262\n",
      "Learning Rate 6.904935435159132e-05\n",
      "Average loss epoch 290: 0.07575908005237579\n",
      "Average latent loss epoch 290: 0.0026212453842163084\n",
      "Learning Rate 6.896123522892594e-05\n",
      "Average loss epoch 291: 0.07732416987419129\n",
      "Average latent loss epoch 291: 0.005372768640518189\n",
      "Learning Rate 6.887322524562478e-05\n",
      "Average loss epoch 292: 0.08549991101026536\n",
      "Average latent loss epoch 292: 0.004251724481582642\n",
      "Learning Rate 6.878532440168783e-05\n",
      "Average loss epoch 293: 0.08136602640151977\n",
      "Average latent loss epoch 293: 0.004090535640716553\n",
      "Learning Rate 6.86975326971151e-05\n",
      "Average loss epoch 294: 0.06906967461109162\n",
      "Average latent loss epoch 294: 0.0038343340158462523\n",
      "Learning Rate 6.86098646838218e-05\n",
      "Average loss epoch 295: 0.0914926677942276\n",
      "Average latent loss epoch 295: 0.004283544421195984\n",
      "Learning Rate 6.85222985339351e-05\n",
      "Average loss epoch 296: 0.08327307850122452\n",
      "Average latent loss epoch 296: 0.006562259793281555\n",
      "Learning Rate 6.843484152341262e-05\n",
      "Average loss epoch 297: 0.06768091470003128\n",
      "Average latent loss epoch 297: 0.0037015467882156373\n",
      "Learning Rate 6.834750820416957e-05\n",
      "Average loss epoch 298: 0.0769231602549553\n",
      "Average latent loss epoch 298: 0.004945668578147888\n",
      "Learning Rate 6.826028402429074e-05\n",
      "Average loss epoch 299: 0.07501331716775894\n",
      "Average latent loss epoch 299: 0.004807859659194946\n",
      "Learning Rate 6.817316170781851e-05\n",
      "Average loss epoch 300: 0.09013986885547638\n",
      "Average latent loss epoch 300: 0.004392403364181519\n",
      "Learning Rate 6.80861558066681e-05\n",
      "Average loss epoch 301: 0.0846586436033249\n",
      "Average latent loss epoch 301: 0.004632443189620972\n",
      "Learning Rate 6.799925904488191e-05\n",
      "Average loss epoch 302: 0.08951560109853744\n",
      "Average latent loss epoch 302: 0.004198700189590454\n",
      "Learning Rate 6.791247869841754e-05\n",
      "Average loss epoch 303: 0.08322167247533799\n",
      "Average latent loss epoch 303: 0.0023981064558029177\n",
      "Learning Rate 6.782580749131739e-05\n",
      "Average loss epoch 304: 0.083061183989048\n",
      "Average latent loss epoch 304: 0.005667942762374878\n",
      "Learning Rate 6.773923814762384e-05\n",
      "Average loss epoch 305: 0.08088183701038361\n",
      "Average latent loss epoch 305: 0.0038449585437774656\n",
      "Learning Rate 6.765279249520972e-05\n",
      "Average loss epoch 306: 0.07584477812051774\n",
      "Average latent loss epoch 306: 0.0032236248254776\n",
      "Learning Rate 6.756644870620221e-05\n",
      "Average loss epoch 307: 0.08080528452992439\n",
      "Average latent loss epoch 307: 0.005206608772277832\n",
      "Learning Rate 6.748021405655891e-05\n",
      "Average loss epoch 308: 0.08040414750576019\n",
      "Average latent loss epoch 308: 0.005130308866500855\n",
      "Learning Rate 6.739409582223743e-05\n",
      "Average loss epoch 309: 0.08140345066785812\n",
      "Average latent loss epoch 309: 0.0034217774868011475\n",
      "Learning Rate 6.730808672728017e-05\n",
      "Average loss epoch 310: 0.08486970365047455\n",
      "Average latent loss epoch 310: 0.003531914949417114\n",
      "Learning Rate 6.72221794957295e-05\n",
      "Average loss epoch 311: 0.077862249314785\n",
      "Average latent loss epoch 311: 0.003557857871055603\n",
      "Learning Rate 6.713638867950067e-05\n",
      "Average loss epoch 312: 0.07987049967050552\n",
      "Average latent loss epoch 312: 0.0033547103404998778\n",
      "Learning Rate 6.705070700263605e-05\n",
      "Average loss epoch 313: 0.0733851209282875\n",
      "Average latent loss epoch 313: 0.003692379593849182\n",
      "Learning Rate 6.696513446513563e-05\n",
      "Average loss epoch 314: 0.07551491782069206\n",
      "Average latent loss epoch 314: 0.003229987621307373\n",
      "Learning Rate 6.687967106699944e-05\n",
      "Average loss epoch 315: 0.07721243053674698\n",
      "Average latent loss epoch 315: 0.0038358509540557862\n",
      "Learning Rate 6.679430953226984e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 316: 0.08462637215852738\n",
      "Average latent loss epoch 316: 0.0062766939401626585\n",
      "Learning Rate 6.670906441286206e-05\n",
      "Average loss epoch 317: 0.07513352781534195\n",
      "Average latent loss epoch 317: 0.004522556066513061\n",
      "Learning Rate 6.66239284328185e-05\n",
      "Average loss epoch 318: 0.08381607383489609\n",
      "Average latent loss epoch 318: 0.005536183714866638\n",
      "Learning Rate 6.653890159213915e-05\n",
      "Average loss epoch 319: 0.08083256483078002\n",
      "Average latent loss epoch 319: 0.0037731558084487917\n",
      "Learning Rate 6.64539766148664e-05\n",
      "Average loss epoch 320: 0.0836752787232399\n",
      "Average latent loss epoch 320: 0.004055127501487732\n",
      "Learning Rate 6.636916805291548e-05\n",
      "Average loss epoch 321: 0.07983216941356659\n",
      "Average latent loss epoch 321: 0.0036393433809280397\n",
      "Learning Rate 6.628446135437116e-05\n",
      "Average loss epoch 322: 0.07236837148666382\n",
      "Average latent loss epoch 322: 0.004128482937812805\n",
      "Learning Rate 6.619986379519105e-05\n",
      "Average loss epoch 323: 0.07597070932388306\n",
      "Average latent loss epoch 323: 0.003340023756027222\n",
      "Learning Rate 6.611538265133277e-05\n",
      "Average loss epoch 324: 0.0726534977555275\n",
      "Average latent loss epoch 324: 0.004320505261421204\n",
      "Learning Rate 6.603100337088108e-05\n",
      "Average loss epoch 325: 0.07932862937450409\n",
      "Average latent loss epoch 325: 0.0033133238554000854\n",
      "Learning Rate 6.594673322979361e-05\n",
      "Average loss epoch 326: 0.08486274927854538\n",
      "Average latent loss epoch 326: 0.0037757933139801027\n",
      "Learning Rate 6.586256495211273e-05\n",
      "Average loss epoch 327: 0.07770733088254929\n",
      "Average latent loss epoch 327: 0.003936776518821716\n",
      "Learning Rate 6.577851308975369e-05\n",
      "Average loss epoch 328: 0.07168307825922966\n",
      "Average latent loss epoch 328: 0.003836807608604431\n",
      "Learning Rate 6.569456309080124e-05\n",
      "Average loss epoch 329: 0.08766304850578308\n",
      "Average latent loss epoch 329: 0.004387027025222779\n",
      "Learning Rate 6.561071495525539e-05\n",
      "Average loss epoch 330: 0.07741463482379914\n",
      "Average latent loss epoch 330: 0.004899445176124573\n",
      "Learning Rate 6.552698323503137e-05\n",
      "Average loss epoch 331: 0.07337695956230164\n",
      "Average latent loss epoch 331: 0.003235068917274475\n",
      "Learning Rate 6.544335337821394e-05\n",
      "Average loss epoch 332: 0.07837823033332825\n",
      "Average latent loss epoch 332: 0.003917214274406433\n",
      "Learning Rate 6.535983266076073e-05\n",
      "Average loss epoch 333: 0.08646666705608368\n",
      "Average latent loss epoch 333: 0.0039018332958221436\n",
      "Learning Rate 6.527642108267173e-05\n",
      "Average loss epoch 334: 0.07955399453639984\n",
      "Average latent loss epoch 334: 0.0032408684492111206\n",
      "Learning Rate 6.519311136798933e-05\n",
      "Average loss epoch 335: 0.07633545696735382\n",
      "Average latent loss epoch 335: 0.0033204644918441773\n",
      "Learning Rate 6.510990351671353e-05\n",
      "Average loss epoch 336: 0.07730628252029419\n",
      "Average latent loss epoch 336: 0.004263171553611755\n",
      "Learning Rate 6.502680480480194e-05\n",
      "Average loss epoch 337: 0.07762803733348847\n",
      "Average latent loss epoch 337: 0.003910854458808899\n",
      "Learning Rate 6.494381523225456e-05\n",
      "Average loss epoch 338: 0.08329872339963913\n",
      "Average latent loss epoch 338: 0.0035617947578430176\n",
      "Learning Rate 6.48609347990714e-05\n",
      "Average loss epoch 339: 0.07736744061112404\n",
      "Average latent loss epoch 339: 0.0041468143463134766\n",
      "Learning Rate 6.477815622929484e-05\n",
      "Average loss epoch 340: 0.07248543798923493\n",
      "Average latent loss epoch 340: 0.0037495702505111693\n",
      "Learning Rate 6.469547952292487e-05\n",
      "Average loss epoch 341: 0.07711746394634247\n",
      "Average latent loss epoch 341: 0.003101697564125061\n",
      "Learning Rate 6.461291195591912e-05\n",
      "Average loss epoch 342: 0.07512373924255371\n",
      "Average latent loss epoch 342: 0.0037804901599884032\n",
      "Learning Rate 6.453044625231996e-05\n",
      "Average loss epoch 343: 0.0842178076505661\n",
      "Average latent loss epoch 343: 0.004161745309829712\n",
      "Learning Rate 6.444808968808502e-05\n",
      "Average loss epoch 344: 0.0831084206700325\n",
      "Average latent loss epoch 344: 0.003355845808982849\n",
      "Learning Rate 6.436584226321429e-05\n",
      "Average loss epoch 345: 0.07369193583726882\n",
      "Average latent loss epoch 345: 0.0033019334077835083\n",
      "Learning Rate 6.428369670175016e-05\n",
      "Average loss epoch 346: 0.07375895008444786\n",
      "Average latent loss epoch 346: 0.003867331147193909\n",
      "Learning Rate 6.420165300369263e-05\n",
      "Average loss epoch 347: 0.07269108146429062\n",
      "Average latent loss epoch 347: 0.0031685113906860353\n",
      "Learning Rate 6.411971844499931e-05\n",
      "Average loss epoch 348: 0.07884349524974824\n",
      "Average latent loss epoch 348: 0.0036978214979171754\n",
      "Learning Rate 6.403787847375497e-05\n",
      "Average loss epoch 349: 0.06859195008873939\n",
      "Average latent loss epoch 349: 0.005173510313034058\n",
      "Learning Rate 6.395615491783246e-05\n",
      "Average loss epoch 350: 0.07577491328120231\n",
      "Average latent loss epoch 350: 0.0029369056224822997\n",
      "Learning Rate 6.387452594935894e-05\n",
      "Average loss epoch 351: 0.08409401178359985\n",
      "Average latent loss epoch 351: 0.0032648563385009764\n",
      "Learning Rate 6.379301339620724e-05\n",
      "Average loss epoch 352: 0.08809450119733811\n",
      "Average latent loss epoch 352: 0.003838500380516052\n",
      "Learning Rate 6.371159543050453e-05\n",
      "Average loss epoch 353: 0.07919050753116608\n",
      "Average latent loss epoch 353: 0.0032237857580184937\n",
      "Learning Rate 6.363027932820842e-05\n",
      "Average loss epoch 354: 0.07463154047727585\n",
      "Average latent loss epoch 354: 0.002599582076072693\n",
      "Learning Rate 6.354907236527652e-05\n",
      "Average loss epoch 355: 0.07357609272003174\n",
      "Average latent loss epoch 355: 0.0022675424814224245\n",
      "Learning Rate 6.346797454170883e-05\n",
      "Average loss epoch 356: 0.08083663582801819\n",
      "Average latent loss epoch 356: 0.004695025086402893\n",
      "Learning Rate 6.338697130559012e-05\n",
      "Average loss epoch 357: 0.07758601605892182\n",
      "Average latent loss epoch 357: 0.0036913812160491942\n",
      "Learning Rate 6.330607720883563e-05\n",
      "Average loss epoch 358: 0.07496932744979859\n",
      "Average latent loss epoch 358: 0.002776312828063965\n",
      "Learning Rate 6.322527769953012e-05\n",
      "Average loss epoch 359: 0.07434822916984558\n",
      "Average latent loss epoch 359: 0.00392647385597229\n",
      "Learning Rate 6.314458732958883e-05\n",
      "Average loss epoch 360: 0.07923865616321564\n",
      "Average latent loss epoch 360: 0.0068379431962966915\n",
      "Learning Rate 6.306399882305413e-05\n",
      "Average loss epoch 361: 0.07066043317317963\n",
      "Average latent loss epoch 361: 0.0033317118883132934\n",
      "Learning Rate 6.298351945588365e-05\n",
      "Average loss epoch 362: 0.07647703737020492\n",
      "Average latent loss epoch 362: 0.002156546711921692\n",
      "Learning Rate 6.290312740020454e-05\n",
      "Average loss epoch 363: 0.07242956459522247\n",
      "Average latent loss epoch 363: 0.002533280849456787\n",
      "Learning Rate 6.282285175984725e-05\n",
      "Average loss epoch 364: 0.07727790027856826\n",
      "Average latent loss epoch 364: 0.0036101013422012327\n",
      "Learning Rate 6.274267070693895e-05\n",
      "Average loss epoch 365: 0.07663816213607788\n",
      "Average latent loss epoch 365: 0.00441550612449646\n",
      "Learning Rate 6.266259879339486e-05\n",
      "Average loss epoch 366: 0.0765071727335453\n",
      "Average latent loss epoch 366: 0.0042734891176223755\n",
      "Learning Rate 6.258262146729976e-05\n",
      "Average loss epoch 367: 0.07635106071829796\n",
      "Average latent loss epoch 367: 0.0017433464527130127\n",
      "Learning Rate 6.250275328056887e-05\n",
      "Average loss epoch 368: 0.0774485632777214\n",
      "Average latent loss epoch 368: 0.0031594067811965944\n",
      "Learning Rate 6.242297968128696e-05\n",
      "Average loss epoch 369: 0.08032828271389007\n",
      "Average latent loss epoch 369: 0.004608479142189026\n",
      "Learning Rate 6.234331522136927e-05\n",
      "Average loss epoch 370: 0.07931858599185944\n",
      "Average latent loss epoch 370: 0.0036774426698684692\n",
      "Learning Rate 6.226375262485817e-05\n",
      "Average loss epoch 371: 0.08082463145256043\n",
      "Average latent loss epoch 371: 0.005909821391105652\n",
      "Learning Rate 6.218429189175367e-05\n",
      "Average loss epoch 372: 0.07886600494384766\n",
      "Average latent loss epoch 372: 0.002234959602355957\n",
      "Learning Rate 6.210492574609816e-05\n",
      "Average loss epoch 373: 0.0786602482199669\n",
      "Average latent loss epoch 373: 0.002954721450805664\n",
      "Learning Rate 6.202566146384925e-05\n",
      "Average loss epoch 374: 0.07565226703882218\n",
      "Average latent loss epoch 374: 0.0020160406827926637\n",
      "Learning Rate 6.194650632096455e-05\n",
      "Average loss epoch 375: 0.07348404675722123\n",
      "Average latent loss epoch 375: 0.004590040445327759\n",
      "Learning Rate 6.186744576552883e-05\n",
      "Average loss epoch 376: 0.07554420977830886\n",
      "Average latent loss epoch 376: 0.0036351025104522706\n",
      "Learning Rate 6.178848707349971e-05\n",
      "Average loss epoch 377: 0.0685280829668045\n",
      "Average latent loss epoch 377: 0.0034016996622085573\n",
      "Learning Rate 6.170963024487719e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 378: 0.07300230413675309\n",
      "Average latent loss epoch 378: 0.002866929769515991\n",
      "Learning Rate 6.163087527966127e-05\n",
      "Average loss epoch 379: 0.06910627186298371\n",
      "Average latent loss epoch 379: 0.001651439070701599\n",
      "Learning Rate 6.155222217785195e-05\n",
      "Average loss epoch 380: 0.07600217163562775\n",
      "Average latent loss epoch 380: 0.0034081369638442995\n",
      "Learning Rate 6.14736636634916e-05\n",
      "Average loss epoch 381: 0.07452253848314286\n",
      "Average latent loss epoch 381: 0.0023041486740112303\n",
      "Learning Rate 6.139519973658025e-05\n",
      "Average loss epoch 382: 0.07864928394556045\n",
      "Average latent loss epoch 382: 0.004288312792778015\n",
      "Learning Rate 6.131685222499073e-05\n",
      "Average loss epoch 383: 0.07080718576908111\n",
      "Average latent loss epoch 383: 0.003179189562797546\n",
      "Learning Rate 6.123859202489257e-05\n",
      "Average loss epoch 384: 0.08409742563962937\n",
      "Average latent loss epoch 384: 0.003989794850349426\n",
      "Learning Rate 6.116044096415862e-05\n",
      "Average loss epoch 385: 0.07486334517598152\n",
      "Average latent loss epoch 385: 0.003384438157081604\n",
      "Learning Rate 6.108238449087366e-05\n",
      "Average loss epoch 386: 0.07723064869642257\n",
      "Average latent loss epoch 386: 0.002159509062767029\n",
      "Learning Rate 6.1004429880995303e-05\n",
      "Average loss epoch 387: 0.07674140110611916\n",
      "Average latent loss epoch 387: 0.003251117467880249\n",
      "Learning Rate 6.092656622058712e-05\n",
      "Average loss epoch 388: 0.07179999351501465\n",
      "Average latent loss epoch 388: 0.0031707972288131715\n",
      "Learning Rate 6.084881169954315e-05\n",
      "Average loss epoch 389: 0.06847212612628936\n",
      "Average latent loss epoch 389: 0.00256553590297699\n",
      "Learning Rate 6.077115540392697e-05\n",
      "Average loss epoch 390: 0.07650669366121292\n",
      "Average latent loss epoch 390: 0.0039022743701934816\n",
      "Learning Rate 6.0693590057780966e-05\n",
      "Average loss epoch 391: 0.074359480291605\n",
      "Average latent loss epoch 391: 0.002566304802894592\n",
      "Learning Rate 6.0616133850999177e-05\n",
      "Average loss epoch 392: 0.07842990756034851\n",
      "Average latent loss epoch 392: 0.0033774048089981077\n",
      "Learning Rate 6.053877223166637e-05\n",
      "Average loss epoch 393: 0.0706574872136116\n",
      "Average latent loss epoch 393: 0.0023248463869094847\n",
      "Learning Rate 6.046150883776136e-05\n",
      "Average loss epoch 394: 0.07854533195495605\n",
      "Average latent loss epoch 394: 0.0035232424736022947\n",
      "Learning Rate 6.0384343669284135e-05\n",
      "Average loss epoch 395: 0.08031906709074974\n",
      "Average latent loss epoch 395: 0.0027068257331848145\n",
      "Learning Rate 6.030728036421351e-05\n",
      "Average loss epoch 396: 0.07812071293592453\n",
      "Average latent loss epoch 396: 0.00393650233745575\n",
      "Learning Rate 6.023031164659187e-05\n",
      "Average loss epoch 397: 0.07603966370224953\n",
      "Average latent loss epoch 397: 0.004427683353424072\n",
      "Learning Rate 6.015344843035564e-05\n",
      "Average loss epoch 398: 0.06232550591230392\n",
      "Average latent loss epoch 398: 0.003194728493690491\n",
      "Learning Rate 6.007667616358958e-05\n",
      "Average loss epoch 399: 0.06501408517360688\n",
      "Average latent loss epoch 399: 0.002209511399269104\n",
      "Learning Rate 6.000000212225132e-05\n"
     ]
    }
   ],
   "source": [
    "num_batches = int(num_train_samples/batch_size)\n",
    "global_step_op = tf.train.get_global_step()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.\n",
    "        epoch_latent_loss = 0.\n",
    "        for batch in range(num_batches):\n",
    "            batch_num = sess.run(global_step_op)\n",
    "            _ , loss, scalar_summaries_, x_out_, x_in_,learning_rate_,latent_loss_ = sess.run([train_step, total_loss, scalar_summaries, x_out, x_in,learning_rate, latent_loss],feed_dict=feed_dict(batch_size))\n",
    "            train_summary_writer.add_summary(scalar_summaries_, global_step=batch_num)\n",
    "            epoch_loss += loss\n",
    "            epoch_latent_loss += latent_loss_\n",
    "            \n",
    "            sigma_sq_enc_ = sess.run(sigma_sq_enc, feed_dict=feed_dict(batch_size))\n",
    "            #Tracer()()\n",
    "            \n",
    "            \n",
    "            #print('Epoch Loss: {}'.format(loss))\n",
    "        print('Average loss epoch {0}: {1}'.format(epoch, epoch_loss/num_batches)) \n",
    "        print('Average latent loss epoch {0}: {1}'.format(epoch, epoch_latent_loss/num_batches)) \n",
    "        print('Learning Rate {}'.format(learning_rate_))\n",
    "        sess.run(increment_epoch_num_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_x_io(x, samp_num):\n",
    "    x_arr = np.asarray(x)\n",
    "    plt.imshow(x_arr[:,samp_num,:].T)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAAFTCAYAAACAt6olAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181ace0780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAAFTCAYAAACAt6olAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFkJgIYT/B4bR5XD0YO/dAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181ace06d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_x_io(x_in_, 0)\n",
    "plot_x_io(x_out_, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
