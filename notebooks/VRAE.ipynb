{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from jupyterthemes import jtplot\n",
    "from IPython.core.debugger import Tracer\n",
    "jtplot.style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def clip_roll(piano_roll, time_steps=50):\n",
    "    samples = []\n",
    "    num_samples = int(piano_roll.shape[1] / time_steps)\n",
    "    for i in range(num_samples):\n",
    "        start_idx = time_steps*i\n",
    "        end_idx = (time_steps*(i+1))\n",
    "        samples.append(piano_roll[:,start_idx:end_idx])\n",
    "    return samples   \n",
    "\n",
    "def create_samples(load_root, time_steps=50, verbose=False):\n",
    "    if not os.path.isdir(load_root):\n",
    "        print(\"Invalid load_root directory.\")\n",
    "        sys.exit(0)\n",
    "        \n",
    "    samples = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(load_root):\n",
    "        for file in filenames:\n",
    "            if file.endswith('.npy'):\n",
    "                load_filepath = os.path.join(dirpath,file)\n",
    "                if verbose:\n",
    "                    print(load_filepath)\n",
    "                piano_roll = np.load(load_filepath).T\n",
    "                samples = samples + clip_roll(piano_roll,time_steps=time_steps)\n",
    "\n",
    "    return np.stack(samples)\n",
    "\n",
    "def feed_dict(batch_size):\n",
    "    indeces = np.random.randint(num_train_samples, size=batch_size)\n",
    "    return {X: np.take(samples, indeces, axis=0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X \\in {0,1}^{batch_size, dim_x, time_steps}\n",
    "time_steps = 10 #TEST\n",
    "x_in_dim = 88\n",
    "z_dim = 20 #TEST\n",
    "num_hidden_units = 500\n",
    "batch_size = 2#100 #TEST\n",
    "#learning_rate = 5*1e-6\n",
    "beta_1 = 0.05\n",
    "beta_2 = 0.001\n",
    "num_epochs = 400\n",
    "initial_learning_rate = 1e-4\n",
    "decay_rate = .6\n",
    "\n",
    "load_root = '../MIDI_Data_PianoRolls/Nottingham/train/'\n",
    "save_root = ''\n",
    "samples = create_samples(load_root, time_steps=time_steps, verbose=False)\n",
    "num_train_samples = 10 #TEST #samples.shape[0] #TEST only pick 20 samples and overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nottingham', 'train']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_root.split('/')[-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(batch_size, x_in_dim, time_steps))\n",
    "\n",
    "# time_slices containts input x at time t across batches.\n",
    "x_in = time_steps * [None]\n",
    "x_out = time_steps * [None]\n",
    "h_enc = time_steps * [None]\n",
    "h_dec = (time_steps + 1) * [None]\n",
    "\n",
    "for t in range(time_steps):\n",
    "    x_in[t] = tf.squeeze(tf.slice(X,begin=[0,0,t],size=[-1,-1,1]),axis=2)\n",
    "\n",
    "###### Encoder network ###########\n",
    "with tf.variable_scope('encoder_rnn'):\n",
    "    cell_enc = tf.nn.rnn_cell.BasicRNNCell(num_hidden_units,activation=tf.nn.tanh)\n",
    "    h_enc[0] = tf.zeros([batch_size,num_hidden_units], dtype=tf.float32) # Initial state is 0\n",
    "\n",
    "    # h_t+1 = tanh(Wenc*h_t + Win*x_t+1 + b )\n",
    "    #Most basic RNN: output = new_state = act(W * input + U * state + B).\n",
    "    #https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/rnn_cell_impl.py\n",
    "    for t in range(time_steps-1):\n",
    "        _ , h_enc[t+1] = cell_enc(inputs=x_in[t+1], state=h_enc[t])\n",
    "\n",
    "\n",
    "mu_enc = tf.layers.dense(h_enc[-1], z_dim, activation=None, name='mu_enc')\n",
    "log_sigma_enc = tf.layers.dense(h_enc[-1], z_dim, activation=None, name='log_sigma_enc')\n",
    "\n",
    "###### Reparametrize ##############\n",
    "eps = tf.random_normal(tf.shape(log_sigma_enc))\n",
    "z = mu_enc + tf.exp(log_sigma_enc) * eps\n",
    "\n",
    "##### Decoder network ############\n",
    "with tf.variable_scope('decoder_rnn'):\n",
    "    W_out = tf.get_variable('W_out',shape=[num_hidden_units, x_in_dim])\n",
    "    b_out = tf.get_variable('b_out',shape=[x_in_dim])\n",
    "    \n",
    "    cell_dec = tf.nn.rnn_cell.BasicRNNCell(num_hidden_units,activation=tf.nn.tanh)\n",
    "    h_dec[0] = tf.layers.dense(z, num_hidden_units, activation=tf.nn.tanh)\n",
    "    \n",
    "    for t in range(time_steps):\n",
    "        x_out[t] = tf.nn.sigmoid(tf.matmul(h_dec[t], W_out) + b_out)\n",
    "        if t < time_steps - 1:\n",
    "            _, h_dec[t+1] = cell_dec(inputs=x_out[t], state=h_dec[t])\n",
    "\n",
    "##### Loss #####################\n",
    "with tf.variable_scope('loss'):\n",
    "    # Latent loss: -KL[q(z|x)|p(z)]\n",
    "    with tf.variable_scope('latent_loss'):\n",
    "        sigma_sq_enc = tf.square(tf.exp(log_sigma_enc))\n",
    "        latent_loss = -.5 * tf.reduce_mean(tf.reduce_sum((1 + tf.log(1e-10 + sigma_sq_enc)) - tf.square(mu_enc) - sigma_sq_enc, axis=1),axis=0)\n",
    "        latent_loss_summ = tf.summary.scalar('latent_loss',latent_loss)\n",
    "        \n",
    "    # Reconstruction Loss: log(p(x|z))    \n",
    "    with tf.variable_scope('recon_loss'):    \n",
    "        for i in range(time_steps):\n",
    "            if i == 0:\n",
    "                recon_loss_ = x_in[i] * tf.log(1e-10 + x_out[i]) + (1 - x_in[i]) * tf.log(1e-10+1-x_out[i])\n",
    "            else:\n",
    "                recon_loss_ += x_in[i] * tf.log(1e-10 + x_out[i]) + (1 - x_in[i]) * tf.log(1e-10+1-x_out[i])\n",
    "            \n",
    "        #collapse the loss, mean across a sample across all x_dim and time points, mean over batches\n",
    "        recon_loss = -tf.reduce_mean(tf.reduce_mean(recon_loss_/(time_steps),axis=1),axis=0)\n",
    "\n",
    "            \n",
    "    recon_loss_summ = tf.summary.scalar('recon_loss', recon_loss)\n",
    "                \n",
    "    with tf.variable_scope('total_loss'):\n",
    "        total_loss = latent_loss + recon_loss\n",
    "    \n",
    "    total_loss_summ = tf.summary.scalar('total_loss', total_loss)\n",
    "\n",
    "global_step = tf.Variable(0,name='global_step') \n",
    "epoch_num = tf.Variable(1, name='epoch_num', trainable=False, dtype=tf.int32)\n",
    "increment_epoch_num_op = tf.assign(epoch_num, epoch_num+1)\n",
    "\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate, epoch_num, num_epochs, decay_rate, staircase=False)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta_1, beta2=beta_2).minimize(total_loss,global_step=global_step)    \n",
    "scalar_summaries = tf.summary.merge([latent_loss_summ, recon_loss_summ, total_loss_summ])\n",
    "#image_summaries = tf.summary.merge()\n",
    "\n",
    "train_summary_writer = tf.summary.FileWriter('./logs', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = int(num_train_samples/batch_size)\n",
    "global_step_op = tf.train.get_global_step()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.\n",
    "        epoch_latent_loss = 0.\n",
    "        for batch in range(num_batches):\n",
    "            batch_num = sess.run(global_step_op)\n",
    "            _ , loss, scalar_summaries_, x_out_, x_in_,learning_rate_,latent_loss_ = sess.run([train_step, total_loss, scalar_summaries, x_out, x_in,learning_rate, latent_loss],feed_dict=feed_dict(batch_size))\n",
    "            train_summary_writer.add_summary(scalar_summaries_, global_step=batch_num)\n",
    "            epoch_loss += loss\n",
    "            epoch_latent_loss += latent_loss_\n",
    "            \n",
    "            sigma_sq_enc_ = sess.run(sigma_sq_enc, feed_dict=feed_dict(batch_size))\n",
    "            #Tracer()()\n",
    "            \n",
    "            \n",
    "            #print('Epoch Loss: {}'.format(loss))\n",
    "        print('Average loss epoch {0}: {1}'.format(epoch, epoch_loss/num_batches)) \n",
    "        print('Average latent loss epoch {0}: {1}'.format(epoch, epoch_latent_loss/num_batches)) \n",
    "        print('Learning Rate {}'.format(learning_rate_))\n",
    "        sess.run(increment_epoch_num_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_x_io(x, samp_num):\n",
    "    x_arr = np.asarray(x)\n",
    "    plt.imshow(x_arr[:,samp_num,:].T)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x_io(x_in_, 0)\n",
    "plot_x_io(x_out_, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
